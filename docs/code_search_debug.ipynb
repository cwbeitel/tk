{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code search debug setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example setup for debugging aspects of the code_search example. This should make it easier to debug and write tests for various aspects of the model as well as provide a simple interface for exploring its performance during development.\n",
    "\n",
    "Fairly similar to [hello_t2t.ipynb](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb).\n",
    "\n",
    "Currently appears to require tf 1.6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "from six import StringIO\n",
    "import tempfile\n",
    "\n",
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.layers import common_layers\n",
    "from tensor2tensor.models import transformer\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.utils import t2t_model\n",
    "\n",
    "from tensor2tensor.data_generators import generator_utils\n",
    "from tensor2tensor.data_generators import text_problems\n",
    "from tensor2tensor.utils import metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tk\n",
    "\n",
    "from tensorflow.contrib.eager.python import tfe\n",
    "tfe.enable_eager_execution()\n",
    "Modes = tf.estimator.ModeKeys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@registry.register_problem\n",
    "class GithubFunctionDocstring(text_problems.Text2TextProblem):\n",
    "  \"\"\"Function and Docstring similarity Problem.\n",
    "  This problem contains the data consisting of function\n",
    "  and docstring pairs as CSV files. The files are structured\n",
    "  such that they contain two columns without headers containing\n",
    "  the docstring tokens and function tokens. The delimiter is\n",
    "  \",\".\n",
    "  \"\"\"\n",
    "\n",
    "  DATA_PATH_PREFIX = \"gs://kubeflow-examples/t2t-code-search/raw_data\"\n",
    "\n",
    "  @property\n",
    "  def pair_files_list(self):\n",
    "    \"\"\"Return URL and file names.\n",
    "    This format is a convention across the Tensor2Tensor (T2T)\n",
    "    codebase. It should be noted that the file names are currently\n",
    "    hardcoded. This is to preserve the semantics of a T2T problem.\n",
    "    In case a change of these values is desired, one must subclass\n",
    "    and override this property.\n",
    "    # TODO(sanyamkapoor): Manually separate train/eval data set.\n",
    "    Returns:\n",
    "      A list of the format,\n",
    "        [\n",
    "          [\n",
    "            \"STRING\",\n",
    "            (\"STRING\", \"STRING\", ...)\n",
    "          ],\n",
    "          ...\n",
    "        ]\n",
    "      Each element is a list of size 2 where the first represents\n",
    "      the source URL and the next is an n-tuple of file names.\n",
    "      In this case, the tuple is of size 1 because the URL points\n",
    "      to a file itself.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        [\n",
    "            \"{}/func-doc-pairs-000{:02}-of-00100.csv\".format(\n",
    "                self.DATA_PATH_PREFIX, i),\n",
    "            (\"func-doc-pairs-000{:02}-of-00100.csv\".format(i),)\n",
    "        ]\n",
    "        for i in range(1)\n",
    "    ]\n",
    "\n",
    "  @property\n",
    "  def is_generate_per_split(self):\n",
    "    return False\n",
    "\n",
    "  @property\n",
    "  def approx_vocab_size(self):\n",
    "    return 2**13\n",
    "\n",
    "  @property\n",
    "  def max_samples_for_vocab(self):\n",
    "    # FIXME(sanyamkapoor): This exists to handle memory explosion.\n",
    "    return int(2e5)\n",
    "\n",
    "  def get_csv_files(self, _data_dir, tmp_dir, _dataset_split):\n",
    "    return [\n",
    "        generator_utils.maybe_download(tmp_dir, file_list[0], uri)\n",
    "        for uri, file_list in self.pair_files_list\n",
    "    ]\n",
    "\n",
    "  def generate_samples(self, data_dir, tmp_dir, dataset_split):\n",
    "    \"\"\"A generator to return data samples.Returns the data generator to return.\n",
    "    Args:\n",
    "      data_dir: A string representing the data directory.\n",
    "      tmp_dir: A string representing the temporary directory and is\n",
    "              used to download files if not already available.\n",
    "      dataset_split: Train, Test or Eval.\n",
    "    Yields:\n",
    "      Each element yielded is of a Python dict of the form\n",
    "        {\"inputs\": \"STRING\", \"targets\": \"STRING\", \"embed_code\": [0]}\n",
    "    \"\"\"\n",
    "    csv_files = self.get_csv_files(data_dir, tmp_dir, dataset_split)\n",
    "\n",
    "    for pairs_file in csv_files:\n",
    "      tf.logging.debug(\"Reading {}\".format(pairs_file))\n",
    "      with tf.gfile.Open(pairs_file) as csv_file:\n",
    "        for line in csv_file:\n",
    "          reader = csv.reader(StringIO(line))\n",
    "          for docstring_tokens, function_tokens in reader:\n",
    "            yield {\n",
    "                \"inputs\": docstring_tokens,\n",
    "                \"targets\": function_tokens,\n",
    "                \"embed_code\": [0]\n",
    "            }\n",
    "\n",
    "  def example_reading_spec(self):\n",
    "    data_fields, data_items_to_decoders = super(GithubFunctionDocstring,\n",
    "                                                self).example_reading_spec()\n",
    "    data_fields[\"embed_code\"] = tf.FixedLenFeature([1], dtype=tf.int64)\n",
    "\n",
    "    data_items_to_decoders = {\n",
    "      \"inputs\": tf.contrib.slim.tfexample_decoder.Tensor(tensor_key=\"inputs\"),\n",
    "      \"targets\": tf.contrib.slim.tfexample_decoder.Tensor(tensor_key=\"targets\"),\n",
    "      \"embed_code\": tf.contrib.slim.tfexample_decoder.Tensor(tensor_key=\"embed_code\")\n",
    "    }\n",
    "    return data_fields, data_items_to_decoders\n",
    "\n",
    "  def eval_metrics(self):  # pylint: disable=no-self-use\n",
    "    return [\n",
    "        metrics.Metrics.ACC\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your paths here!\n",
    "\n",
    "tmp_dir = \"/mnt/nfs-east1-d/cs/tmp\"\n",
    "tf.gfile.MakeDirs(tmp_dir)\n",
    "\n",
    "data_dir = \"/mnt/nfs-east1-d/cs/data\"\n",
    "tf.gfile.MakeDirs(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found vocab file: /mnt/nfs-east1-d/cs/data/vocab.github_function_docstring.8192.subwords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-09-30 17:11:52,665] Found vocab file: /mnt/nfs-east1-d/cs/data/vocab.github_function_docstring.8192.subwords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Skipping generator because outputs files exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-09-30 17:11:52,745] Skipping generator because outputs files exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Skipping shuffle because output files exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-09-30 17:11:52,759] Skipping shuffle because output files exist\n"
     ]
    }
   ],
   "source": [
    "\n",
    "problem_object = GithubFunctionDocstring()\n",
    "\n",
    "problem_object.generate_data(data_dir, tmp_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading data files from /mnt/nfs-east1-d/cs/data/github_function_docstring-train*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-09-30 17:11:52,853] Reading data files from /mnt/nfs-east1-d/cs/data/github_function_docstring-train*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:partition: 0 num_data_files: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-09-30 17:11:52,959] partition: 0 num_data_files: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embed_code': <tf.Tensor: id=52, shape=(1,), dtype=int64, numpy=array([0])>,\n",
      " 'inputs': <tf.Tensor: id=53, shape=(11,), dtype=int64, numpy=array([ 300,   14, 7463,   44, 1686, 1717, 5561,    4,  115,   18,    1])>,\n",
      " 'targets': <tf.Tensor: id=54, shape=(17,), dtype=int64, numpy=\n",
      "array([   7,  300,    2, 7463,   48,    3,   60,  121,    9,    3, 7463,\n",
      "         48,    2,  222,   60,  121,    1])>}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "example = tfe.Iterator(problem_object.dataset(Modes.TRAIN, data_dir)).next()\n",
    "\n",
    "pprint.pprint(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_name = \"vocab.github_function_docstring.8192.subwords\"\n",
    "\n",
    "# Get the encoders from the problem\n",
    "encoders = problem_object.feature_encoders(data_dir)\n",
    "\n",
    "# Setup helper functions for encoding and decoding\n",
    "def encode(input_str, output_str=None):\n",
    "  \"\"\"Input str to features dict, ready for inference\"\"\"\n",
    "  inputs = encoders[\"inputs\"].encode(input_str) + [1]  # add EOS id\n",
    "  batch_inputs = tf.reshape(inputs, [1, -1, 1])  # Make it 3D.\n",
    "  return {\"inputs\": batch_inputs}\n",
    "\n",
    "def decode(integers):\n",
    "  \"\"\"List of ints to str\"\"\"\n",
    "  integers = list(np.squeeze(integers))\n",
    "  if 1 in integers:\n",
    "    integers = integers[:integers.index(1)]\n",
    "  return encoders[\"inputs\"].decode(np.squeeze(integers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make a monte carlo sampler object .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(decode(example[\"inputs\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def make_monty self args kwargs return self monty_cls args kwargs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(decode(example[\"targets\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FYI: You can break these losses out and work with them interactively in tf.eager mode\n",
    "# the same as you would a regular python function!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rank_loss(sentence_emb, image_emb, margin=0.2):\n",
    "  \"\"\"Experimental rank loss, thanks to kkurach@ for the code.\"\"\"\n",
    "  with tf.name_scope(\"rank_loss\"):\n",
    "    # Normalize first as this is assumed in cosine similarity later.\n",
    "    sentence_emb = tf.nn.l2_normalize(sentence_emb, 1)\n",
    "    image_emb = tf.nn.l2_normalize(image_emb, 1)\n",
    "    # Both sentence_emb and image_emb have size [batch, depth].\n",
    "    scores = tf.matmul(image_emb, tf.transpose(sentence_emb))  # [batch, batch]\n",
    "    diagonal = tf.diag_part(scores)  # [batch]\n",
    "    cost_s = tf.maximum(0.0, margin - diagonal + scores)  # [batch, batch]\n",
    "    cost_im = tf.maximum(\n",
    "        0.0, margin - tf.reshape(diagonal, [-1, 1]) + scores)  # [batch, batch]\n",
    "    # Clear diagonals.\n",
    "    batch_size = tf.shape(sentence_emb)[0]\n",
    "    empty_diagonal_mat = tf.ones_like(cost_s) - tf.eye(batch_size)\n",
    "    cost_s *= empty_diagonal_mat\n",
    "    cost_im *= empty_diagonal_mat\n",
    "\n",
    "    return cost_s + cost_im\n",
    "    \n",
    "    return cost_s, cost_im\n",
    "    \n",
    "    return tf.reduce_mean(cost_s) + tf.reduce_mean(cost_im)\n",
    "\n",
    "def slicenet_similarity_cost(inputs_encoded, targets_encoded):\n",
    "  \"\"\"Loss telling to be more similar to your own targets than to others.\"\"\"\n",
    "  # This is a first very simple version: handle variable-length by padding\n",
    "  # to same length and putting everything into batch. In need of a better way.\n",
    "  x, y = common_layers.pad_to_same_length(inputs_encoded, targets_encoded)\n",
    "  return rank_loss(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kf_loss(string_embedding, code_embedding):\n",
    "\n",
    "  string_embedding_norm = tf.nn.l2_normalize(string_embedding, axis=1)\n",
    "  code_embedding_norm = tf.nn.l2_normalize(code_embedding, axis=1)\n",
    "  tf.logging.debug(\"string_embedding_norm: %s\" % string_embedding_norm)\n",
    "  tf.logging.debug(\"code_embedding_norm: %s\" % code_embedding_norm)\n",
    "\n",
    "  # All-vs-All cosine distance matrix, reshaped as row-major.\n",
    "  cosine_dist = 1.0 - tf.matmul(string_embedding_norm, code_embedding_norm,\n",
    "                                transpose_b=True)\n",
    "  cosine_dist_flat = tf.reshape(cosine_dist, [-1, 1])\n",
    "  tf.logging.debug(\"cosine_dist_flat: %s\" % cosine_dist_flat)\n",
    "\n",
    "  # Positive samples on the diagonal, reshaped as row-major.\n",
    "  label_matrix = tf.eye(tf.shape(cosine_dist)[0], dtype=tf.int32)\n",
    "  label_matrix_flat = tf.reshape(label_matrix, [-1])\n",
    "  tf.logging.debug(\"label_matrix_flat: %s\" % label_matrix_flat)\n",
    "\n",
    "  logits = tf.concat([1.0 - cosine_dist_flat, cosine_dist_flat], axis=1)\n",
    "  tf.logging.debug(\"logits: %s\" % logits)\n",
    "\n",
    "  labels = tf.one_hot(label_matrix_flat, 2)\n",
    "  tf.logging.debug(\"labels: %s\" % labels)\n",
    "\n",
    "  loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels,\n",
    "                                                 logits=logits)\n",
    "  tf.logging.debug(\"loss: %s\" % loss)\n",
    "\n",
    "  return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [3. 4. 3.]\n",
      " [5. 6. 3.]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[4. 3. 3.]\n",
      " [2. 1. 3.]\n",
      " [7. 8. 3.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "t1 = tf.convert_to_tensor(\n",
    "    np.array([[1.0, 2.0, 3.0], [3.0, 4.0, 3.0], [5.0, 6.0, 3.0]], dtype=np.float32),\n",
    "    dtype=tf.float32)\n",
    "\n",
    "t2 = tf.convert_to_tensor(\n",
    "    np.array([[4.0, 3.0, 3.0], [2.0, 1.0, 3.0], [7.0, 8.0, 3.0]], dtype=np.float32),\n",
    "    dtype=tf.float32)\n",
    "\n",
    "print(t1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1.3132617  0.69314724]\n",
      "  [0.33635572 0.735665  ]\n",
      "  [0.32775706 0.7199305 ]]\n",
      "\n",
      " [[0.33635572 0.735665  ]\n",
      "  [1.3132616  0.6931471 ]\n",
      "  [0.3868397  0.8258845 ]]\n",
      "\n",
      " [[0.32775706 0.7199305 ]\n",
      "  [0.3868397  0.8258845 ]\n",
      "  [1.3132617  0.69314724]]], shape=(3, 3, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[1.2205269  0.6306621 ]\n",
      "  [0.33297884 0.7294991 ]\n",
      "  [0.37914097 0.8123544 ]]\n",
      "\n",
      " [[0.32125714 0.70796114]\n",
      "  [1.2205269  0.6306621 ]\n",
      "  [0.32344216 0.7119921 ]]\n",
      "\n",
      " [[0.32323566 0.71161145]\n",
      "  [0.37153625 0.7989113 ]\n",
      "  [1.3100034  0.6909199 ]]], shape=(3, 3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.reshape(kf_loss(t2, t2), (3,3,2)))\n",
    "print(tf.reshape(kf_loss(t1, t2), (3,3,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# So just a little confused here why we have a vector of two values for the distance between a pair of embeddings?\n",
    "# Each value in these pairs almost exactly sum to 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kf_loss_mod(string_embedding, code_embedding):\n",
    "\n",
    "  string_embedding_norm = tf.nn.l2_normalize(string_embedding, axis=1)\n",
    "  code_embedding_norm = tf.nn.l2_normalize(code_embedding, axis=1)\n",
    "  tf.logging.debug(\"string_embedding_norm: %s\" % string_embedding_norm)\n",
    "  tf.logging.debug(\"code_embedding_norm: %s\" % code_embedding_norm)\n",
    "\n",
    "  # All-vs-All cosine distance matrix, reshaped as row-major.\n",
    "  cosine_dist = 1.0 - tf.matmul(string_embedding_norm, code_embedding_norm,\n",
    "                                transpose_b=True)\n",
    "  cosine_dist_flat = tf.reshape(cosine_dist, [-1, 1])\n",
    "  tf.logging.debug(\"cosine_dist_flat: %s\" % cosine_dist_flat)\n",
    "\n",
    "  # Positive samples on the diagonal, reshaped as row-major.\n",
    "  label_matrix = tf.eye(tf.shape(cosine_dist)[0], dtype=tf.int32)\n",
    "  label_matrix_flat = tf.reshape(label_matrix, [-1])\n",
    "  tf.logging.debug(\"label_matrix_flat: %s\" % label_matrix_flat)\n",
    "\n",
    "  #logits = tf.concat([1.0 - cosine_dist_flat, cosine_dist_flat], axis=1)\n",
    "  logits = tf.maximum(0.0, cosine_dist_flat)\n",
    "  tf.logging.debug(\"logits: %s\" % logits)\n",
    "\n",
    "  labels = tf.one_hot(label_matrix_flat, 1)\n",
    "  tf.logging.debug(\"labels: %s\" % labels)\n",
    "\n",
    "  loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels,\n",
    "                                                 logits=logits)\n",
    "  tf.logging.debug(\"loss: %s\" % loss)\n",
    "\n",
    "  return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.6931472  0.6523636  0.6670625 ]\n",
      " [0.6523636  0.69314724 0.57598215]\n",
      " [0.6670625  0.57598215 0.6931472 ]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.75979847 0.65807045 0.5866487 ]\n",
      " [0.67854947 0.75979847 0.67465085]\n",
      " [0.6750176  0.59750694 0.69537944]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Makes more sense, but still shouldn't values on diag be zero? Perhaps I missed something.\n",
    "\n",
    "print(tf.reshape(kf_loss_mod(t2, t2), (3,3)))\n",
    "print(tf.reshape(kf_loss_mod(t1, t2), (3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=99935, shape=(3, 3), dtype=float32, numpy=\n",
      "array([[0.        , 0.11669868, 0.14713186],\n",
      "       [0.11669844, 0.        , 0.        ],\n",
      "       [0.14713186, 0.        , 0.        ]], dtype=float32)>, <tf.Tensor: id=99936, shape=(3, 3), dtype=float32, numpy=\n",
      "array([[0.        , 0.11669844, 0.14713186],\n",
      "       [0.11669868, 0.        , 0.        ],\n",
      "       [0.14713186, 0.        , 0.        ]], dtype=float32)>)\n",
      "(<tf.Tensor: id=99980, shape=(3, 3), dtype=float32, numpy=\n",
      "array([[0.        , 0.2997247 , 0.1678657 ],\n",
      "       [0.25770772, 0.        , 0.00305521],\n",
      "       [0.10343069, 0.29179513, 0.        ]], dtype=float32)>, <tf.Tensor: id=99981, shape=(3, 3), dtype=float32, numpy=\n",
      "array([[0.        , 0.2997247 , 0.29254252],\n",
      "       [0.25770772, 0.        , 0.12773204],\n",
      "       [0.        , 0.16711831, 0.        ]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The slicenet similarity cost produces a single loss value instead of pairwise distances\n",
    "\n",
    "print(slicenet_similarity_cost(t2, t2))\n",
    "print(slicenet_similarity_cost(t1, t2))\n",
    "\n",
    "# If this is summed we'll get the total distance from all elements to their non-pairs, ignoring self-distances.\n",
    "# Perhaps we should also consider self-distances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _get_initializer(_):\n",
    "  return None\n",
    "\n",
    "class SimilarityTransformer(t2t_model.T2TModel):\n",
    "  \"\"\"Transformer Model for Similarity between two strings.\n",
    "  This model defines the architecture using two transformer\n",
    "  networks, each of which embed a string and the loss is\n",
    "  calculated as a Binary Cross-Entropy loss. Normalized\n",
    "  Dot Product is used as the distance measure between two\n",
    "  string embeddings.\n",
    "  \"\"\"\n",
    "\n",
    "  def top(self, body_output, _):  # pylint: disable=no-self-use\n",
    "    return body_output\n",
    "\n",
    "  def body(self, features):\n",
    "        \n",
    "    loss_variant = self.hparams.loss_variant\n",
    "\n",
    "    initializer = _get_initializer(self.hparams.initializer)\n",
    "    docs_encoder_trainable = self.hparams.docs_encoder_trainable\n",
    "    code_encoder_trainable = self.hparams.code_encoder_trainable\n",
    "\n",
    "    with tf.variable_scope('string_embedding'):\n",
    "      string_embedding = self.encode(features, 'inputs')\n",
    "      tf.logging.debug(\"string_embedding: %s\" % string_embedding)\n",
    "\n",
    "    if 'targets' in features:\n",
    "      with tf.variable_scope('code_embedding'):\n",
    "        code_embedding = self.encode(features, 'targets')\n",
    "        tf.logging.debug(\"code_embedding: %s\" % code_embedding)\n",
    "    \n",
    "      loss = self.loss(string_embedding, code_embedding, loss_variant)\n",
    "    \n",
    "      return string_embedding, {\"training\": loss}\n",
    "    \n",
    "    return string_embedding, {\"training\": 0.0}\n",
    "    \n",
    "  def loss(self, string_embedding, code_embedding, loss_variant):\n",
    "    \"\"\"Compute either the kfnet or slicenet cosine similarity loss.\"\"\"\n",
    "\n",
    "    if loss_variant == \"slicenet\":\n",
    "      loss = slicenet_similarity_cost(string_embedding, code_embedding)\n",
    "    elif loss_variant == \"kfnet\":\n",
    "      loss = kf_loss(string_embedding, code_embedding)\n",
    "    else:\n",
    "      raise ValueError(\"Unrecognize loss variant: %s\" % loss_variant)\n",
    "    return loss\n",
    "\n",
    "  def encode(self, features, input_key):\n",
    "    hparams = self._hparams\n",
    "    inputs = common_layers.flatten4d3d(features[input_key])\n",
    "\n",
    "    (encoder_input, encoder_self_attention_bias, _) = (\n",
    "        transformer.transformer_prepare_encoder(inputs, problem.SpaceID.EN_TOK,\n",
    "                                                hparams))\n",
    "\n",
    "    encoder_input = tf.nn.dropout(encoder_input,\n",
    "                                  1.0 - hparams.layer_prepostprocess_dropout)\n",
    "    encoder_output = transformer.transformer_encoder(\n",
    "        encoder_input,\n",
    "        encoder_self_attention_bias,\n",
    "        hparams,\n",
    "        nonpadding=transformer.features_to_nonpadding(features, input_key))\n",
    "\n",
    "    encoder_output = tf.reduce_mean(encoder_output, axis=1)\n",
    "\n",
    "    return encoder_output\n",
    "\n",
    "  def infer(self, features=None, **kwargs):\n",
    "    del kwargs\n",
    "\n",
    "    predictions, _ = self(features)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensor2tensor.models.transformer import transformer_base\n",
    "\n",
    "def similarity_transformer_tiny():\n",
    "  hparams = transformer_base()\n",
    "  hparams.num_hidden_layers = 2\n",
    "  hparams.hidden_size = 128\n",
    "  hparams.filter_size = 512\n",
    "  hparams.num_heads = 4\n",
    "  hparams.docs_encoder_trainable = True\n",
    "  hparams.code_encoder_trainable = True\n",
    "  hparams.initializer = None\n",
    "  hparams.loss_variant = \"kfnet\"\n",
    "  return hparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting T2TModel mode to 'train'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-09-30 17:11:53,513] Setting T2TModel mode to 'train'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading data files from /mnt/nfs-east1-d/cs/data/github_function_docstring-train*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-09-30 17:11:53,517] Reading data files from /mnt/nfs-east1-d/cs/data/github_function_docstring-train*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:partition: 0 num_data_files: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-09-30 17:11:53,622] partition: 0 num_data_files: 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hparams = similarity_transformer_tiny()\n",
    "hparams.data_dir = data_dir\n",
    "\n",
    "p_hparams = problem_object.get_hparams(hparams)\n",
    "\n",
    "model = SimilarityTransformer(\n",
    "    hparams, tf.estimator.ModeKeys.TRAIN, p_hparams\n",
    ")\n",
    "\n",
    "batch_size = 1\n",
    "train_dataset = problem_object.dataset(Modes.TRAIN, data_dir)\n",
    "train_dataset = train_dataset.repeat(None).batch(batch_size)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and examine result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_8185_128.bottom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-09-30 17:11:53,838] Transforming feature 'inputs' with symbol_modality_8185_128.bottom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Transforming 'targets' with symbol_modality_8185_128.targets_bottom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-09-30 17:11:54,046] Transforming 'targets' with symbol_modality_8185_128.targets_bottom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Building model body\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-09-30 17:11:54,056] Building model body\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Skipping T2TModel top and loss because training loss returned from body\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-09-30 17:11:54,304] Skipping T2TModel top and loss because training loss returned from body\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 0.521\n",
      "Step: 1, Loss: 0.382\n",
      "Step: 2, Loss: 0.307\n",
      "Step: 3, Loss: 0.289\n",
      "Step: 4, Loss: 0.262\n",
      "Step: 5, Loss: 0.252\n",
      "Step: 6, Loss: 0.251\n",
      "Step: 7, Loss: 0.249\n",
      "Step: 8, Loss: 0.246\n",
      "Step: 9, Loss: 0.239\n",
      "Step: 10, Loss: 0.243\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@tfe.implicit_value_and_gradients\n",
    "def loss_fn(features):\n",
    "  _, losses = model(features)\n",
    "  return losses[\"training\"]\n",
    "\n",
    "NUM_STEPS = 10\n",
    "\n",
    "for count, example in enumerate(tfe.Iterator(train_dataset)):\n",
    "  loss, gv = loss_fn(example)\n",
    "  optimizer.apply_gradients(gv)\n",
    "\n",
    "  if count % 1 == 0:\n",
    "   print(\"Step: %d, Loss: %.3f\" % (count, loss.numpy()))\n",
    "  if count >= NUM_STEPS:\n",
    "   break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starts monitoring network notifications and recording http responses .\n",
      "def StartMonitoringNetwork self self ClearResponseData self _inspector_websocket RegisterDomain Network self _OnNetworkNotification request method Network enable self _inspector_websocket SyncRequest request\n",
      "tf.Tensor(\n",
      "[[ 0.7172416   0.92741555  0.12348276 -0.20051704 -1.1058692   0.86038166\n",
      "  -1.2172744  -1.3298419   0.745497   -0.7550367  -0.5932518  -1.4806631\n",
      "   0.51983285  2.0449784  -1.5088761  -0.03365893 -0.04511219 -1.5894246\n",
      "   0.5326434  -0.15864295  1.1758411  -0.36474767  1.169934    1.35163\n",
      "  -0.68106705  0.37723333  1.5279374  -0.6439409   1.1116624  -1.3295312\n",
      "  -0.2936      1.3362228   1.5472255   0.69980896 -0.38871604 -0.75966763\n",
      "  -1.0516654   0.21517804 -0.68333787 -1.1471457  -0.4602326  -0.90439755\n",
      "   1.0481467  -0.4632301   0.44710428 -0.31434947  0.11345819 -0.8577829\n",
      "   2.047798    1.849212    0.83297217  1.0712117   0.01263916  0.16294779\n",
      "  -1.6924348   0.34745318 -0.5920931  -0.00282303  1.4379827   1.2685648\n",
      "   0.5572372  -0.47432387 -0.8787298  -1.2523605   0.43582553 -0.03982605\n",
      "  -1.171925   -0.03156656  0.16268024  0.20871828 -1.8749715   1.231881\n",
      "   0.04343405 -1.1466601   0.49357864 -0.81759864 -1.0872605   0.43209314\n",
      "   0.2467176   0.5664384   0.55628335 -1.79271    -0.3517144  -1.2960815\n",
      "  -0.25661913 -0.00950066  1.2609054   0.50136787 -0.49579668  1.3572885\n",
      "  -0.6965053   1.5499637   0.7631103   0.48662132  0.49756348 -1.1084802\n",
      "   0.50245005  0.68427116  1.4349906   0.5294686  -1.2174406  -0.69948924\n",
      "  -0.32913443 -0.4541477  -0.7223845  -0.585871    0.295759   -0.02945508\n",
      "   0.01469632  0.83158785 -0.80873615  0.08937953  0.06199192 -1.1561704\n",
      "   1.8292726  -0.5187636  -1.0476617   0.54972017 -1.1025183  -0.64750326\n",
      "  -0.2848444   1.2976964  -0.07066263 -0.58832884  1.5069823  -0.44018343\n",
      "  -0.8798096   0.2885494 ]], shape=(1, 128), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 7.08830416e-01  9.59957778e-01  2.59350210e-01 -3.25375736e-01\n",
      "  -1.01346457e+00  7.66593754e-01 -1.30521452e+00 -9.29190397e-01\n",
      "   9.74998236e-01 -5.69825172e-01 -6.26125991e-01 -1.63055968e+00\n",
      "   5.70797145e-01  1.61083043e+00 -1.89593554e+00 -3.37788314e-01\n",
      "   2.60486871e-01 -1.69937849e+00  6.59676671e-01 -2.32827678e-01\n",
      "   1.17587030e+00 -1.97461769e-01  1.31875122e+00  1.23877203e+00\n",
      "  -1.05760306e-01  4.66397643e-01  1.15484869e+00 -5.73187590e-01\n",
      "   1.22266436e+00 -1.26666129e+00 -3.48913223e-01  1.43716550e+00\n",
      "   1.59122348e+00  8.61291528e-01  1.19754032e-03 -6.34198010e-01\n",
      "  -1.37515748e+00  2.25602493e-01 -7.22838461e-01 -9.41629648e-01\n",
      "  -1.36725962e-01 -1.29893780e+00  9.41083372e-01 -4.71266240e-01\n",
      "   4.41422284e-01 -6.01690650e-01  2.73266792e-01 -6.79800510e-01\n",
      "   1.92636979e+00  2.11189985e+00  9.89028633e-01  1.00055552e+00\n",
      "   2.47176379e-01  1.88229710e-01 -1.63329065e+00  5.08577488e-02\n",
      "  -2.25900352e-01  1.38117924e-01  1.31356382e+00  9.05627370e-01\n",
      "   1.00407910e+00 -4.12294298e-01 -4.65574384e-01 -1.02000296e+00\n",
      "   7.30154455e-01  1.62465170e-01 -1.04644203e+00 -3.62701148e-01\n",
      "   3.39492112e-01  1.74172863e-01 -1.66280580e+00  1.30310869e+00\n",
      "   1.22165633e-02 -8.67950976e-01  5.87014675e-01 -5.12762606e-01\n",
      "  -1.14903319e+00  2.21712053e-01  1.72065243e-01  6.89689159e-01\n",
      "  -3.97373326e-02 -1.83514404e+00 -6.73583090e-01 -1.48553681e+00\n",
      "  -4.25534457e-01 -1.72524899e-01  1.14882970e+00  2.70933002e-01\n",
      "  -7.27333009e-01  1.07137179e+00 -8.19279552e-01  1.51845610e+00\n",
      "   8.36504340e-01  4.17879015e-01  1.41855806e-01 -1.13290548e+00\n",
      "   5.18598557e-01  6.60776556e-01  1.30304658e+00  2.68152624e-01\n",
      "  -1.14245582e+00 -9.15288746e-01 -2.81506449e-01 -4.37986791e-01\n",
      "  -3.72984409e-01 -5.90919316e-01  6.58471406e-01  3.54663432e-01\n",
      "  -5.18278740e-02  8.66345942e-01 -5.14252722e-01  3.17841619e-01\n",
      "   1.36166543e-01 -1.60118413e+00  1.63090634e+00 -2.56195515e-01\n",
      "  -1.45839298e+00  3.79483432e-01 -9.65634525e-01 -3.85187447e-01\n",
      "  -2.81540394e-01  1.29387271e+00 -6.36131465e-02 -9.95290399e-01\n",
      "   1.34380567e+00 -5.34311235e-01 -1.25211859e+00  7.66215334e-03]], shape=(1, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "example = tfe.Iterator(train_dataset).next()\n",
    "\n",
    "print(decode(example[\"inputs\"]))\n",
    "print(decode(example[\"targets\"]))\n",
    "\n",
    "print(model.infer({\"inputs\": example[\"inputs\"]}))\n",
    "print(model.infer({\"inputs\": example[\"targets\"]}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine distances between pairs and non-pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "def random_airquote_code_airquote():\n",
    "  codelen = random.randint(10,100)\n",
    "  code = []\n",
    "  for i in range(0, codelen):\n",
    "    N = random.randint(1,10)\n",
    "    code.append(''.join(random.choice(string.ascii_lowercase) for _ in range(N)))\n",
    "  return ' '.join(code)\n",
    "\n",
    "def compare_to_random(query, code):\n",
    "    \n",
    "    code2 = random_airquote_code_airquote()\n",
    "\n",
    "    with tfe.restore_variables_on_create(ckpt_path):\n",
    "\n",
    "      doc_emb = model.infer(encode(query))\n",
    "      code1_emb = model.infer(encode(code1))\n",
    "      code2_emb = model.infer(encode(code2))\n",
    "\n",
    "    dtrue = model.loss(doc_emb, code1_emb, \"kfnet\")\n",
    "    dfalse = model.loss(doc_emb, code2_emb, \"kfnet\")\n",
    "    print(\"Dist for true pair: %s\" % dtrue)\n",
    "    print(\"Dist for false pair: %s\" % dfalse)\n",
    "    \n",
    "    return dtrue, dfalse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dist for true pair: tf.Tensor([[1.2818332 0.6717591]], shape=(1, 2), dtype=float32)\n",
      "Dist for false pair: tf.Tensor([[1.2767197 0.6682996]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"print query\"\n",
    "code1 = \"def my_function(query):  print(query)\"\n",
    "code2 = \"nronfg vmo5i n6565-23 wrdnds vdmam65 3ehn bdp\"\n",
    "\n",
    "doc_emb = model.infer(encode(query))\n",
    "code1_emb = model.infer(encode(code1))\n",
    "code2_emb = model.infer(encode(code2))\n",
    "\n",
    "print(\"Dist for true pair: %s\" % model.loss(doc_emb, code1_emb, \"kfnet\"))\n",
    "print(\"Dist for false pair: %s\" % model.loss(doc_emb, code2_emb, \"kfnet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dist for true pair: tf.Tensor([[1.2711855 0.6645619]], shape=(1, 2), dtype=float32)\n",
      "Dist for false pair: tf.Tensor([[1.2465297  0.64799166]], shape=(1, 2), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=83353, shape=(1, 2), dtype=float32, numpy=array([[1.2711855, 0.6645619]], dtype=float32)>,\n",
       " <tf.Tensor: id=83400, shape=(1, 2), dtype=float32, numpy=array([[1.2465297 , 0.64799166]], dtype=float32)>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_to_random(query, code1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_pair_non_pair_distances():\n",
    "\n",
    "    num_steps = 10\n",
    "    doc_emb_last = None\n",
    "    code_emb_last = None\n",
    "\n",
    "    for step in range(0, num_steps):\n",
    "\n",
    "        example = tfe.Iterator(train_dataset).next()\n",
    "\n",
    "        #print(decode(example[\"inputs\"]))\n",
    "        #print(decode(example[\"targets\"]))\n",
    "\n",
    "        doc_emb = model.infer({\"inputs\": example[\"inputs\"]})\n",
    "        code_emb = model.infer({\"inputs\": example[\"targets\"]})\n",
    "\n",
    "        print(\"Dist for true pair: %s\" % model.loss(doc_emb, code_emb, \"kfnet\"))\n",
    "\n",
    "        if doc_emb_last is not None:\n",
    "          print(\"Dist for false pair: %s\" % model.loss(doc_emb, code_emb_last, \"kfnet\"))\n",
    "          # Assuming examples are well shuffled...\n",
    "\n",
    "        print('\\n')\n",
    "        doc_emb_last = doc_emb\n",
    "        code_emb_last = code_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dist for true pair: tf.Tensor([[1.2899535  0.67726463]], shape=(1, 2), dtype=float32)\n",
      "\n",
      "\n",
      "Dist for true pair: tf.Tensor([[1.2899222 0.6772435]], shape=(1, 2), dtype=float32)\n",
      "Dist for false pair: tf.Tensor([[1.2910588 0.6780152]], shape=(1, 2), dtype=float32)\n",
      "\n",
      "\n",
      "Dist for true pair: tf.Tensor([[1.2851241  0.67398864]], shape=(1, 2), dtype=float32)\n",
      "Dist for false pair: tf.Tensor([[1.2964038  0.68164814]], shape=(1, 2), dtype=float32)\n",
      "\n",
      "\n",
      "Dist for true pair: tf.Tensor([[1.2921448  0.67875284]], shape=(1, 2), dtype=float32)\n",
      "Dist for false pair: tf.Tensor([[1.2831182 0.6726295]], shape=(1, 2), dtype=float32)\n",
      "\n",
      "\n",
      "Dist for true pair: tf.Tensor([[1.3063977  0.68845767]], shape=(1, 2), dtype=float32)\n",
      "Dist for false pair: tf.Tensor([[1.3056698  0.68796104]], shape=(1, 2), dtype=float32)\n",
      "\n",
      "\n",
      "Dist for true pair: tf.Tensor([[1.2883658 0.6761871]], shape=(1, 2), dtype=float32)\n",
      "Dist for false pair: tf.Tensor([[1.289762  0.6771347]], shape=(1, 2), dtype=float32)\n",
      "\n",
      "\n",
      "Dist for true pair: tf.Tensor([[1.281821  0.6717509]], shape=(1, 2), dtype=float32)\n",
      "Dist for false pair: tf.Tensor([[1.2766623 0.6682608]], shape=(1, 2), dtype=float32)\n",
      "\n",
      "\n",
      "Dist for true pair: tf.Tensor([[1.301272  0.6849625]], shape=(1, 2), dtype=float32)\n",
      "Dist for false pair: tf.Tensor([[1.2982606 0.6829117]], shape=(1, 2), dtype=float32)\n",
      "\n",
      "\n",
      "Dist for true pair: tf.Tensor([[1.2951511  0.68079615]], shape=(1, 2), dtype=float32)\n",
      "Dist for false pair: tf.Tensor([[1.2922087  0.67879623]], shape=(1, 2), dtype=float32)\n",
      "\n",
      "\n",
      "Dist for true pair: tf.Tensor([[1.2735832 0.6661805]], shape=(1, 2), dtype=float32)\n",
      "Dist for false pair: tf.Tensor([[1.2883599  0.67618304]], shape=(1, 2), dtype=float32)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "show_pair_non_pair_distances()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate a long training run\n",
    "\n",
    "Assuming relevant model and problem versions are in t2t_usr_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train locally, e.g. using tiny hparams to check that things run okay.\n",
    "\n",
    "!t2t-trainer --t2t_usr_dir=/mnt/nfs-east1-d/work/tk/tk \\\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Your favorite way to launch jobs here, e.g. Faring, ksonnet, a Python\n",
    "# wrapper around ksonnet, etc.\n",
    "\n",
    "# In my case I'm currently using this (unsupported) library\n",
    "\n",
    "args = tk.configure_experiment(\"cs-dev0\",\n",
    "                             problem=\"github_function_docstring\",\n",
    "                             num_gpu_per_worker=1,\n",
    "                             hparams_set=\"transformer_tiny\",\n",
    "                             model=\"similarity_transformer\",\n",
    "                             extra_hparams={\n",
    "                             },\n",
    "                             num_steps=10000)\n",
    "\n",
    "job = tk.experiment.T2TExperiment(**args)\n",
    "job.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The path to checkpoints for the newly trained model, accessible to local FS\n",
    "\n",
    "ckpt_path = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the distance for pair and non-pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dist for true pair: tf.Tensor([[1.2757524 0.6676459]], shape=(1, 2), dtype=float32)\n",
      "Dist for false pair: tf.Tensor([[1.2516866  0.65144634]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "query = \"print query\"\n",
    "code = \"def my_function(query):  print(query)\"\n",
    "\n",
    "with tfe.restore_variables_on_create(ckpt_path):\n",
    "    compare_to_random(query, code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dist for true pair: tf.Tensor([[1.2839055  0.67316276]], shape=(1, 2), dtype=float32)\n",
      "\n",
      "\n",
      "Dist for true pair: tf.Tensor([[1.2889736  0.67659956]], shape=(1, 2), dtype=float32)\n",
      "Dist for false pair: tf.Tensor([[1.2806787 0.6709776]], shape=(1, 2), dtype=float32)\n",
      "\n",
      "\n",
      "Dist for true pair: tf.Tensor([[1.291646 0.678414]], shape=(1, 2), dtype=float32)\n",
      "Dist for false pair: tf.Tensor([[1.2848523  0.67380446]], shape=(1, 2), dtype=float32)\n",
      "\n",
      "\n",
      "Dist for true pair: tf.Tensor([[1.2833872 0.6728116]], shape=(1, 2), dtype=float32)\n",
      "Dist for false pair: tf.Tensor([[1.2894423 0.6769176]], shape=(1, 2), dtype=float32)\n",
      "\n",
      "\n",
      "Dist for true pair: tf.Tensor([[1.28843   0.6762306]], shape=(1, 2), dtype=float32)\n",
      "Dist for false pair: tf.Tensor([[1.2856084  0.67431694]], shape=(1, 2), dtype=float32)\n",
      "\n",
      "\n",
      "Dist for true pair: tf.Tensor([[1.2573394 0.65524  ]], shape=(1, 2), dtype=float32)\n",
      "Dist for false pair: tf.Tensor([[1.265234  0.6605499]], shape=(1, 2), dtype=float32)\n",
      "\n",
      "\n",
      "Dist for true pair: tf.Tensor([[1.2984159  0.68301743]], shape=(1, 2), dtype=float32)\n",
      "Dist for false pair: tf.Tensor([[1.2773019 0.6686932]], shape=(1, 2), dtype=float32)\n",
      "\n",
      "\n",
      "Dist for true pair: tf.Tensor([[1.2814064 0.6714702]], shape=(1, 2), dtype=float32)\n",
      "Dist for false pair: tf.Tensor([[1.2907321 0.6777933]], shape=(1, 2), dtype=float32)\n",
      "\n",
      "\n",
      "Dist for true pair: tf.Tensor([[1.2787306 0.6696594]], shape=(1, 2), dtype=float32)\n",
      "Dist for false pair: tf.Tensor([[1.2937149  0.67981976]], shape=(1, 2), dtype=float32)\n",
      "\n",
      "\n",
      "Dist for true pair: tf.Tensor([[1.3027829  0.68599224]], shape=(1, 2), dtype=float32)\n",
      "Dist for false pair: tf.Tensor([[1.2830472 0.6725814]], shape=(1, 2), dtype=float32)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tfe.restore_variables_on_create(ckpt_path):\n",
    "    show_pair_non_pair_distances()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-build the search index with the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: For the sake of model development, it would be nice if this could be done with a single command,\n",
    "# provided the path to new checkpoints, and be triggered from the notebook.\n",
    "\n",
    "def rebuild_index(ckpt_path):\n",
    "  \"\"\"Trigger a re-build of the search index.\n",
    "  \n",
    "  i.e. triggers external infrastructure to re-compute embeddings\n",
    "  \n",
    "  Returns:\n",
    "      index or index ID?\n",
    "  \"\"\"\n",
    "    \n",
    "  index_id = None\n",
    "\n",
    "  return index_id\n",
    "\n",
    "index_id = rebuild_index(ckpt_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run queries against the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: hello world\n",
      "Hits:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def search(query, ckpt_path, index_id):\n",
    "\n",
    "  with tfe.restore_variables_on_create(ckpt_path):\n",
    "    doc_emb = model.infer(encode(query))\n",
    "\n",
    "  # TODO: Search `doc_emb` against index with `index_id`\n",
    "  hits = []\n",
    "\n",
    "  return hits\n",
    "\n",
    "\n",
    "query = \"hello world\"\n",
    "\n",
    "hits = search(query, ckpt_path, None)\n",
    "\n",
    "print(\"Query: %s\" % query)\n",
    "print(\"Hits:\")\n",
    "pprint.pprint(hits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute interpretable quality measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Trigger calculation of an interpretable measure of quality, along\n",
    "# the lines of\n",
    "# https://github.com/kubeflow/examples/issues/254#issuecomment-425606539\n",
    "\n",
    "def compute_quality(ckpt_path, index_id):\n",
    "  pass\n",
    "\n",
    "compute_quality(ckpt_path, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export and serving"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
