{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code search debug setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example setup for debugging aspects of the code_search example. This should make it easier to debug and write tests for various aspects of the model as well as provide a simple interface for exploring its performance during development.\n",
    "\n",
    "Fairly similar to [hello_t2t.ipynb](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import csv\n",
    "from six import StringIO\n",
    "import tempfile\n",
    "\n",
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.layers import common_layers\n",
    "from tensor2tensor.models import transformer\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.utils import t2t_model\n",
    "\n",
    "from tensor2tensor.data_generators import generator_utils\n",
    "from tensor2tensor.data_generators import text_problems\n",
    "from tensor2tensor.utils import metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tk\n",
    "\n",
    "from tensorflow.contrib.eager.python import tfe\n",
    "tfe.enable_eager_execution()\n",
    "Modes = tf.estimator.ModeKeys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@registry.register_problem\n",
    "class GithubFunctionDocstring(text_problems.Text2TextProblem):\n",
    "  \"\"\"Function and Docstring similarity Problem.\n",
    "  This problem contains the data consisting of function\n",
    "  and docstring pairs as CSV files. The files are structured\n",
    "  such that they contain two columns without headers containing\n",
    "  the docstring tokens and function tokens. The delimiter is\n",
    "  \",\".\n",
    "  \"\"\"\n",
    "\n",
    "  DATA_PATH_PREFIX = \"gs://kubeflow-examples/t2t-code-search/raw_data\"\n",
    "\n",
    "  @property\n",
    "  def pair_files_list(self):\n",
    "    \"\"\"Return URL and file names.\n",
    "    This format is a convention across the Tensor2Tensor (T2T)\n",
    "    codebase. It should be noted that the file names are currently\n",
    "    hardcoded. This is to preserve the semantics of a T2T problem.\n",
    "    In case a change of these values is desired, one must subclass\n",
    "    and override this property.\n",
    "    # TODO(sanyamkapoor): Manually separate train/eval data set.\n",
    "    Returns:\n",
    "      A list of the format,\n",
    "        [\n",
    "          [\n",
    "            \"STRING\",\n",
    "            (\"STRING\", \"STRING\", ...)\n",
    "          ],\n",
    "          ...\n",
    "        ]\n",
    "      Each element is a list of size 2 where the first represents\n",
    "      the source URL and the next is an n-tuple of file names.\n",
    "      In this case, the tuple is of size 1 because the URL points\n",
    "      to a file itself.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        [\n",
    "            \"{}/func-doc-pairs-000{:02}-of-00100.csv\".format(\n",
    "                self.DATA_PATH_PREFIX, i),\n",
    "            (\"func-doc-pairs-000{:02}-of-00100.csv\".format(i),)\n",
    "        ]\n",
    "        for i in range(1)\n",
    "    ]\n",
    "\n",
    "  @property\n",
    "  def is_generate_per_split(self):\n",
    "    return False\n",
    "\n",
    "  @property\n",
    "  def approx_vocab_size(self):\n",
    "    return 2**13\n",
    "\n",
    "  @property\n",
    "  def max_samples_for_vocab(self):\n",
    "    # FIXME(sanyamkapoor): This exists to handle memory explosion.\n",
    "    return int(2e5)\n",
    "\n",
    "  def get_csv_files(self, _data_dir, tmp_dir, _dataset_split):\n",
    "    return [\n",
    "        generator_utils.maybe_download(tmp_dir, file_list[0], uri)\n",
    "        for uri, file_list in self.pair_files_list\n",
    "    ]\n",
    "\n",
    "  def generate_samples(self, data_dir, tmp_dir, dataset_split):\n",
    "    \"\"\"A generator to return data samples.Returns the data generator to return.\n",
    "    Args:\n",
    "      data_dir: A string representing the data directory.\n",
    "      tmp_dir: A string representing the temporary directory and is\n",
    "              used to download files if not already available.\n",
    "      dataset_split: Train, Test or Eval.\n",
    "    Yields:\n",
    "      Each element yielded is of a Python dict of the form\n",
    "        {\"inputs\": \"STRING\", \"targets\": \"STRING\", \"embed_code\": [0]}\n",
    "    \"\"\"\n",
    "    csv_files = self.get_csv_files(data_dir, tmp_dir, dataset_split)\n",
    "\n",
    "    for pairs_file in csv_files:\n",
    "      tf.logging.debug(\"Reading {}\".format(pairs_file))\n",
    "      with tf.gfile.Open(pairs_file) as csv_file:\n",
    "        for line in csv_file:\n",
    "          reader = csv.reader(StringIO(line))\n",
    "          for docstring_tokens, function_tokens in reader:\n",
    "            yield {\n",
    "                \"inputs\": docstring_tokens,\n",
    "                \"targets\": function_tokens,\n",
    "                \"embed_code\": [0]\n",
    "            }\n",
    "\n",
    "  def example_reading_spec(self):\n",
    "    data_fields, data_items_to_decoders = super(GithubFunctionDocstring,\n",
    "                                                self).example_reading_spec()\n",
    "    data_fields[\"embed_code\"] = tf.FixedLenFeature([1], dtype=tf.int64)\n",
    "\n",
    "    data_items_to_decoders = {\n",
    "      \"inputs\": tf.contrib.slim.tfexample_decoder.Tensor(tensor_key=\"inputs\"),\n",
    "      \"targets\": tf.contrib.slim.tfexample_decoder.Tensor(tensor_key=\"targets\"),\n",
    "      \"embed_code\": tf.contrib.slim.tfexample_decoder.Tensor(tensor_key=\"embed_code\")\n",
    "    }\n",
    "    return data_fields, data_items_to_decoders\n",
    "\n",
    "  def eval_metrics(self):  # pylint: disable=no-self-use\n",
    "    return [\n",
    "        metrics.Metrics.ACC\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your paths here!\n",
    "\n",
    "tmp_dir = \"/mnt/nfs-east1-d/cs/tmp\"\n",
    "tf.gfile.MakeDirs(tmp_dir)\n",
    "\n",
    "data_dir = \"/mnt/nfs-east1-d/cs/data\"\n",
    "tf.gfile.MakeDirs(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "problem_object = GithubFunctionDocstring()\n",
    "\n",
    "problem_object.generate_data(data_dir, tmp_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading data files from /mnt/nfs-east1-d/cs/data/github_function_docstring-train*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-09-28 20:04:06,708] Reading data files from /mnt/nfs-east1-d/cs/data/github_function_docstring-train*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:partition: 0 num_data_files: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-09-28 20:04:06,802] partition: 0 num_data_files: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embed_code': <tf.Tensor: id=7345, shape=(1,), dtype=int64, numpy=array([0])>,\n",
      " 'inputs': <tf.Tensor: id=7346, shape=(14,), dtype=int64, numpy=\n",
      "array([2292, 2525,   61, 2707,   45, 1359,   24, 7081,  171, 1760,   49,\n",
      "       1211,   18,    1])>,\n",
      " 'targets': <tf.Tensor: id=7347, shape=(271,), dtype=int64, numpy=\n",
      "array([   7,   37,    2, 2292, 2525,   61,    2, 7081,  102,  813,   10,\n",
      "       1651, 4209, 4921,   12, 2965, 1005,   28,  641, 2410,   44,   67,\n",
      "       5145,  792,    5,  127, 4954, 4590,  627,  145, 1853, 1836,   22,\n",
      "        296,  605, 2410,   98,  477, 2847, 1121,  160,  228, 3088, 5555,\n",
      "        169,  444, 1905,  109, 7781,    2,  102,  145, 7081, 1373,   45,\n",
      "        228,  223, 1382,  477, 6201, 2613,  553,  822, 3029, 3029,   67,\n",
      "       7081, 1373,   45,  444,  223, 1382,  477, 6201, 2613,  553,  822,\n",
      "       3029,  822,  228, 7081, 1373,   45,  127,  223, 1382,  477, 6201,\n",
      "       2613,  553,  822, 2099,    4, 3028,    4,  228, 7081, 1373,   45,\n",
      "        296,  223, 1382,  477, 6201, 2613,  553,  822, 3029, 2901,    4,\n",
      "        145, 7081, 1373,   45,   10,  223, 1382,  477, 6201, 2613,  553,\n",
      "        822, 2099,    4, 2901,    4,  444, 7081, 1373,   45,   28,  223,\n",
      "       1382,  477, 6201, 2613,  553,  822, 3029, 1130,   10, 7081, 1373,\n",
      "         45,  145,  223, 1382,  477, 6201, 2613,  553,  822, 2099,    4,\n",
      "       1906,    4,   28, 7081, 1373,   45,  444,  223, 1382,  477, 6201,\n",
      "       2613,  553,  822, 2099,    4, 2745,    4,  228, 7081, 1373,   45,\n",
      "        296,  223, 1382,  477, 6201, 2613,  553,  822, 3029, 6725,   28,\n",
      "       7081, 1373,   45,  296,  223, 1382,  477, 6201, 2613,  553,  822,\n",
      "       3029, 1674,   28, 7081, 1373,   45,  127,  223, 1382,  477, 6201,\n",
      "       2613,  553,  822, 3029, 1435, 4314,   10, 1651, 4209, 4921,   12,\n",
      "       2965, 1005,   28, 4954, 4590,  627,   67, 1905,  109,  127,  641,\n",
      "       2410,   44,  145,  605, 2410,   98,  296, 1853, 1836,   22,  477,\n",
      "       2847, 1121,  160,  228, 3088, 5555,  169,  444, 5145,  792,    5,\n",
      "       7081,    5,   17,    2, 7081,  171, 7781,    2,  102,   99, 2292,\n",
      "       2525,   61,  813, 7081,    5, 4314,    1])>}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "example = tfe.Iterator(problem_object.dataset(Modes.TRAIN, data_dir)).next()\n",
    "\n",
    "pprint.pprint(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_name = \"vocab.github_function_docstring.8192.subwords\"\n",
    "\n",
    "# Get the encoders from the problem\n",
    "encoders = problem_object.feature_encoders(data_dir)\n",
    "\n",
    "# Setup helper functions for encoding and decoding\n",
    "def encode(input_str, output_str=None):\n",
    "  \"\"\"Input str to features dict, ready for inference\"\"\"\n",
    "  inputs = encoders[\"inputs\"].encode(input_str) + [1]  # add EOS id\n",
    "  batch_inputs = tf.reshape(inputs, [1, -1, 1])  # Make it 3D.\n",
    "  return {\"inputs\": batch_inputs}\n",
    "\n",
    "def decode(integers):\n",
    "  \"\"\"List of ints to str\"\"\"\n",
    "  integers = list(np.squeeze(integers))\n",
    "  if 1 in integers:\n",
    "    integers = integers[:integers.index(1)]\n",
    "  return encoders[\"inputs\"].decode(np.squeeze(integers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try multiple times to run ' throw_random '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(decode(example[\"inputs\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def throw_random lengths mask saved None for i in range maxtries try return throw_random_bits lengths mask except MaxtriesException as e saved e continue raise e\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(decode(example[\"targets\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Copying the model into your notebook makes it easier to debug and means you don't\n",
    "# need to repeatedly re-load the entire t2t registry with each change.\n",
    "\n",
    "\n",
    "def _get_initializer(_):\n",
    "  return None\n",
    "\n",
    "\n",
    "@registry.register_model\n",
    "class SimilarityTransformerDev(t2t_model.T2TModel):\n",
    "  \"\"\"Transformer Model for Similarity between two strings.\n",
    "  This model defines the architecture using two transformer\n",
    "  networks, each of which embed a string and the loss is\n",
    "  calculated as a Binary Cross-Entropy loss. Normalized\n",
    "  Dot Product is used as the distance measure between two\n",
    "  string embeddings.\n",
    "  \"\"\"\n",
    "\n",
    "  def top(self, body_output, _):\n",
    "    return body_output\n",
    "\n",
    "  def body(self, features):\n",
    "        \n",
    "    initializer = _get_initializer(self.hparms.initializer)\n",
    "    docs_encoder_trainable = self.hparams.docs_encoder_trainable\n",
    "    code_encoder_trainable = self.hparams.code_encoder_trainable\n",
    "        \n",
    "    with tf.variable_scope('string_embedding', initializer=initializer):\n",
    "      string_embedding = self.encode(features, 'inputs',\n",
    "                                    trainable=docs_encoder_trainable)\n",
    "\n",
    "    # Is this to detect whether we're in training mode?\n",
    "    # Instead could use Modes key.\n",
    "    if 'targets' in features:\n",
    "\n",
    "      with tf.variable_scope('code_embedding', initializer=initializer):\n",
    "        code_embedding = self.encode(features, 'targets',\n",
    "                                    trainable=code_encoder_trainable)\n",
    "\n",
    "      loss = self.loss(string_embedding, code_embedding)\n",
    "\n",
    "      return string_embedding, {'training': loss}\n",
    "\n",
    "    return string_embedding\n",
    "\n",
    "  def distance(self, string_embedding, code_embedding):\n",
    "    string_embedding_norm = tf.nn.l2_normalize(string_embedding, axis=1)\n",
    "    code_embedding_norm = tf.nn.l2_normalize(code_embedding, axis=1)\n",
    "\n",
    "    # All-vs-All cosine distance matrix, reshaped as row-major.\n",
    "    cosine_dist = 1.0 - tf.matmul(string_embedding_norm, code_embedding_norm,\n",
    "                                  transpose_b=True)\n",
    "    return cosine_dist\n",
    "\n",
    "  def loss(self, string_embedding, code_embedding):\n",
    "    cosine_dist = self.distance(string_embedding, code_embedding)\n",
    "    cosine_dist_flat = tf.reshape(cosine_dist, [-1, 1])\n",
    "\n",
    "    # Positive samples on the diagonal, reshaped as row-major.\n",
    "    label_matrix = tf.eye(tf.shape(cosine_dist)[0], dtype=tf.int32)\n",
    "    label_matrix_flat = tf.reshape(label_matrix, [-1])\n",
    "\n",
    "    logits = tf.concat([1.0 - cosine_dist_flat, cosine_dist_flat], axis=1)\n",
    "    labels = tf.one_hot(label_matrix_flat, 2)\n",
    "\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels,\n",
    "                                                   logits=logits)\n",
    "    return loss\n",
    "    \n",
    "  # Change to \"embed\"?\n",
    "  def encode(self, features, input_key):\n",
    "    hparams = self._hparams\n",
    "    inputs = common_layers.flatten4d3d(features[input_key])\n",
    "\n",
    "    (encoder_input, encoder_self_attention_bias, _) = (\n",
    "        transformer.transformer_prepare_encoder(inputs, problem.SpaceID.EN_TOK,\n",
    "                                                hparams))\n",
    "\n",
    "    encoder_input = tf.nn.dropout(encoder_input,\n",
    "                                  1.0 - hparams.layer_prepostprocess_dropout)\n",
    "    encoder_output = transformer.transformer_encoder(\n",
    "        encoder_input,\n",
    "        encoder_self_attention_bias,\n",
    "        hparams,\n",
    "        nonpadding=transformer.features_to_nonpadding(features, input_key))\n",
    "\n",
    "    encoder_output = tf.reduce_mean(encoder_output, axis=1)\n",
    "\n",
    "    return encoder_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensor2tensor.models.transformer import transformer_base\n",
    "\n",
    "def similarity_transformer_tiny():\n",
    "  hparams = transformer_base()\n",
    "  hparams.num_hidden_layers = 2\n",
    "  hparams.hidden_size = 128\n",
    "  hparams.filter_size = 512\n",
    "  hparams.num_heads = 4\n",
    "  hparams.docs_encoder_trainable = True\n",
    "  hparams.code_encoder_trainable = True\n",
    "  hparms.initializer = None\n",
    "  return hparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting T2TModel mode to 'train'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-09-28 20:00:55,412] Setting T2TModel mode to 'train'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading data files from /mnt/nfs-east1-d/cs/data/github_function_docstring-train*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-09-28 20:00:55,417] Reading data files from /mnt/nfs-east1-d/cs/data/github_function_docstring-train*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:partition: 0 num_data_files: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-09-28 20:00:55,560] partition: 0 num_data_files: 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hparams = similarity_transformer_tiny()\n",
    "hparams.data_dir = data_dir\n",
    "\n",
    "p_hparams = problem_object.get_hparams(hparams)\n",
    "\n",
    "model = SimilarityTransformer(\n",
    "    hparams, tf.estimator.ModeKeys.TRAIN, p_hparams\n",
    ")\n",
    "\n",
    "batch_size = 1\n",
    "train_dataset = problem_object.dataset(Modes.TRAIN, data_dir)\n",
    "train_dataset = train_dataset.repeat(None).batch(batch_size)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E2E test that combines train and eval steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tfe.implicit_value_and_gradients\n",
    "def loss_fn(features):\n",
    "  _, losses = model(features)\n",
    "  return losses[\"training\"]\n",
    "\n",
    "NUM_STEPS = 10\n",
    "\n",
    "for count, example in enumerate(tfe.Iterator(train_dataset)):\n",
    "  loss, gv = loss_fn(example)\n",
    "  optimizer.apply_gradients(gv)\n",
    "\n",
    "  if count % 1 == 0:\n",
    "   print(\"Step: %d, Loss: %.3f\" % (count, loss.numpy()))\n",
    "  if count >= NUM_STEPS:\n",
    "   break\n",
    "\n",
    "model.set_mode(Modes.EVAL)\n",
    "dataset = problem_object.dataset(Modes.EVAL, data_dir)\n",
    "\n",
    "example = tfe.Iterator(dataset).next()\n",
    "\n",
    "encoded, _ = model.encode(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Doesn't like data type, maybe add float cast in body.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug distance method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO: Not implemented, break out model.distance to enable easier inspection of calculated distances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"hello world\"\n",
    "code = \"def my_function(query):  print(query)\"\n",
    "\n",
    "d = model.distance(model.embed(encode(query)), model.embed(encode(code)))\n",
    "\n",
    "print(\"Distance: %s\" % d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate a long training run\n",
    "\n",
    "Assuming relevant model and problem versions are in t2t_usr_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train locally, e.g. using tiny hparams to check that things run okay.\n",
    "\n",
    "!t2t-trainer --t2t_usr_dir=/mnt/nfs-east1-d/work/tk/tk \\\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Your favorite way to launch jobs here, e.g. Faring, ksonnet, a Python\n",
    "# wrapper around ksonnet, etc.\n",
    "\n",
    "# In my case I'm currently using this (unsupported) library\n",
    "\n",
    "args = tk.configure_experiment(\"cs-dev0\",\n",
    "                             problem=\"github_function_docstring\",\n",
    "                             num_gpu_per_worker=1,\n",
    "                             hparams_set=\"transformer_tiny\",\n",
    "                             model=\"similarity_transformer\",\n",
    "                             extra_hparams={\n",
    "                             },\n",
    "                             num_steps=10000)\n",
    "\n",
    "job = tk.experiment.T2TExperiment(**args)\n",
    "job.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The path to checkpoints for the newly trained model, accessible to local FS\n",
    "\n",
    "ckpt_path = \"...ckpt-NNN\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the distance for pair and non-pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Artificially construct pair and non-pair\n",
    "\n",
    "pair = {...}\n",
    "non_pair = {...}\n",
    "\n",
    "for ex in [pair, non_pair];\n",
    "  ex[\"inputs\"] = encode(ex[\"inputs\"])\n",
    "  ex[\"targets\"] = encode(ex[\"targets\"])\n",
    "\n",
    "with tfe.restore_variables_on_create(ckpt_path):\n",
    "\n",
    "    pair_input_emb = model.encode(pair[\"inputs\"])\n",
    "    pair_target_emb = model.encode(pair[\"targets\"])\n",
    "    print(\"Dist for true pair: %s\" % model.distance(pair_input_emb, pair_target_emb))\n",
    "\n",
    "    non_pair_input_emb = model.encode(pair[\"inputs\"])\n",
    "    non_pair_target_emb = model.encode(pair[\"targets\"])\n",
    "    print(\"Dist for false pair: %s\" % model.distance(non_pair_input_emb, non_pair_target_emb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-build the search index with the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: For the sake of model development, it would be nice if this could be done with a single command,\n",
    "# provided the path to new checkpoints, and be triggered from the notebook.\n",
    "\n",
    "def rebuild_index(ckpt_path):\n",
    "  \"\"\"Trigger a re-build of the search index.\n",
    "  \n",
    "  i.e. triggers external infrastructure to re-compute embeddings\n",
    "  \n",
    "  Returns:\n",
    "      index or index ID?\n",
    "  \"\"\"\n",
    "  return index_id?\n",
    "\n",
    "index_id = rebuild_index(ckpt_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run queries against the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search(query, ckpt_path, index_id)\n",
    "  encoded_inputs = encode(query)\n",
    "  with tfe.restore_variables_on_create(ckpt_path):\n",
    "    query_embedding = model.encode(encoded_inputs)[\"outputs\"]\n",
    "    \n",
    "  # TODO: Search `query_embedding` against index with `index_id`\n",
    "\n",
    "  return hits\n",
    "\n",
    "\n",
    "query = \"hello world\"\n",
    "\n",
    "hits = search(query, ckpt_path)\n",
    "\n",
    "print(\"Query: %s\" % query)\n",
    "print(\"Hits:\")\n",
    "pprint.pprint(hits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute interpretable quality measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Trigger calculation of an interpretable measure of quality, along\n",
    "# the lines of\n",
    "# https://github.com/kubeflow/examples/issues/254#issuecomment-425606539\n",
    "\n",
    "def compute_quality(ckpt_path, index_id):\n",
    "  pass\n",
    "\n",
    "compute_quality()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
