{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve data quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Toward improving quality of training examples, adding additional labels, expanding date range (and number of examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/scipy/optimize/_minimize.py:32: ImportWarning: Not importing directory '/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/scipy/optimize/lbfgsb': missing __init__.py\n",
      "  from .lbfgsb import _minimize_lbfgsb\n",
      "/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/scipy/spatial/__init__.py:95: ImportWarning: Not importing directory '/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/scipy/spatial/qhull': missing __init__.py\n",
      "  from .qhull import *\n",
      "INFO|2018-11-06T22:51:28|/mnt/nfs-east1-d/work/examples/code_search/src/code_search/dataflow/cli/preprocess_github_dataset.py|58| Reading data using a query.\n",
      "INFO|2018-11-06T22:51:28|/mnt/nfs-east1-d/work/examples/code_search/src/code_search/dataflow/transforms/github_dataset.py|64| Writing results to BigQuery kubeflow-rl:github_function_embeddings.token_pairs\n",
      "INFO|2018-11-06T22:51:28|/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/apiclient.py|469| Starting GCS upload to gs://kubeflow-rl-dataflow/cs/staging/preprocess-github-dataset-20181106-225126.1541544688.788675/pipeline.pb...\n",
      "INFO|2018-11-06T22:51:28|/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/oauth2client/transport.py|157| Attempting refresh to obtain initial access_token\n",
      "INFO|2018-11-06T22:51:34|/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/apiclient.py|484| Completed GCS upload to gs://kubeflow-rl-dataflow/cs/staging/preprocess-github-dataset-20181106-225126.1541544688.788675/pipeline.pb\n",
      "INFO|2018-11-06T22:51:34|/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/portability/stager.py|423| Executing command: ['/home/jovyan/.conda/envs/py2/bin/python', 'setup.py', 'sdist', '--dist-dir', '/tmp/tmpuqyhbz']\n",
      "running sdist\n",
      "running egg_info\n",
      "writing requirements to code_search.egg-info/requires.txt\n",
      "writing code_search.egg-info/PKG-INFO\n",
      "writing top-level names to code_search.egg-info/top_level.txt\n",
      "writing dependency_links to code_search.egg-info/dependency_links.txt\n",
      "reading manifest file 'code_search.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "writing manifest file 'code_search.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "running check\n",
      "creating code-search-0.1.dev0\n",
      "creating code-search-0.1.dev0/code_search\n",
      "creating code-search-0.1.dev0/code_search.egg-info\n",
      "creating code-search-0.1.dev0/code_search/dataflow\n",
      "creating code-search-0.1.dev0/code_search/dataflow/cli\n",
      "creating code-search-0.1.dev0/code_search/dataflow/do_fns\n",
      "creating code-search-0.1.dev0/code_search/dataflow/transforms\n",
      "creating code-search-0.1.dev0/code_search/nmslib\n",
      "creating code-search-0.1.dev0/code_search/nmslib/cli\n",
      "creating code-search-0.1.dev0/code_search/t2t\n",
      "copying files to code-search-0.1.dev0...\n",
      "copying MANIFEST.in -> code-search-0.1.dev0\n",
      "copying requirements.txt -> code-search-0.1.dev0\n",
      "copying setup.py -> code-search-0.1.dev0\n",
      "copying code_search/__init__.py -> code-search-0.1.dev0/code_search\n",
      "copying code_search.egg-info/PKG-INFO -> code-search-0.1.dev0/code_search.egg-info\n",
      "copying code_search.egg-info/SOURCES.txt -> code-search-0.1.dev0/code_search.egg-info\n",
      "copying code_search.egg-info/dependency_links.txt -> code-search-0.1.dev0/code_search.egg-info\n",
      "copying code_search.egg-info/requires.txt -> code-search-0.1.dev0/code_search.egg-info\n",
      "copying code_search.egg-info/top_level.txt -> code-search-0.1.dev0/code_search.egg-info\n",
      "copying code_search/dataflow/__init__.py -> code-search-0.1.dev0/code_search/dataflow\n",
      "copying code_search/dataflow/utils.py -> code-search-0.1.dev0/code_search/dataflow\n",
      "copying code_search/dataflow/cli/__init__.py -> code-search-0.1.dev0/code_search/dataflow/cli\n",
      "copying code_search/dataflow/cli/arguments.py -> code-search-0.1.dev0/code_search/dataflow/cli\n",
      "copying code_search/dataflow/cli/create_function_embeddings.py -> code-search-0.1.dev0/code_search/dataflow/cli\n",
      "copying code_search/dataflow/cli/preprocess_github_dataset.py -> code-search-0.1.dev0/code_search/dataflow/cli\n",
      "copying code_search/dataflow/cli/preprocess_github_dataset_test.py -> code-search-0.1.dev0/code_search/dataflow/cli\n",
      "copying code_search/dataflow/do_fns/__init__.py -> code-search-0.1.dev0/code_search/dataflow/do_fns\n",
      "copying code_search/dataflow/do_fns/dict_to_csv.py -> code-search-0.1.dev0/code_search/dataflow/do_fns\n",
      "copying code_search/dataflow/do_fns/function_embeddings.py -> code-search-0.1.dev0/code_search/dataflow/do_fns\n",
      "copying code_search/dataflow/do_fns/github_dataset.py -> code-search-0.1.dev0/code_search/dataflow/do_fns\n",
      "copying code_search/dataflow/do_fns/prediction_do_fn.py -> code-search-0.1.dev0/code_search/dataflow/do_fns\n",
      "copying code_search/dataflow/transforms/__init__.py -> code-search-0.1.dev0/code_search/dataflow/transforms\n",
      "copying code_search/dataflow/transforms/bigquery.py -> code-search-0.1.dev0/code_search/dataflow/transforms\n",
      "copying code_search/dataflow/transforms/function_embeddings.py -> code-search-0.1.dev0/code_search/dataflow/transforms\n",
      "copying code_search/dataflow/transforms/github_bigquery.py -> code-search-0.1.dev0/code_search/dataflow/transforms\n",
      "copying code_search/dataflow/transforms/github_dataset.py -> code-search-0.1.dev0/code_search/dataflow/transforms\n",
      "copying code_search/nmslib/__init__.py -> code-search-0.1.dev0/code_search/nmslib\n",
      "copying code_search/nmslib/search_engine.py -> code-search-0.1.dev0/code_search/nmslib\n",
      "copying code_search/nmslib/search_server.py -> code-search-0.1.dev0/code_search/nmslib\n",
      "copying code_search/nmslib/cli/__init__.py -> code-search-0.1.dev0/code_search/nmslib/cli\n",
      "copying code_search/nmslib/cli/arguments.py -> code-search-0.1.dev0/code_search/nmslib/cli\n",
      "copying code_search/nmslib/cli/create_search_index.py -> code-search-0.1.dev0/code_search/nmslib/cli\n",
      "copying code_search/nmslib/cli/embed_query_test.py -> code-search-0.1.dev0/code_search/nmslib/cli\n",
      "copying code_search/nmslib/cli/start_search_server.py -> code-search-0.1.dev0/code_search/nmslib/cli\n",
      "copying code_search/t2t/__init__.py -> code-search-0.1.dev0/code_search/t2t\n",
      "copying code_search/t2t/function_docstring.py -> code-search-0.1.dev0/code_search/t2t\n",
      "copying code_search/t2t/function_docstring_extended.py -> code-search-0.1.dev0/code_search/t2t\n",
      "copying code_search/t2t/query.py -> code-search-0.1.dev0/code_search/t2t\n",
      "copying code_search/t2t/similarity_transformer.py -> code-search-0.1.dev0/code_search/t2t\n",
      "copying code_search/t2t/similarity_transformer_test.py -> code-search-0.1.dev0/code_search/t2t\n",
      "Writing code-search-0.1.dev0/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'code-search-0.1.dev0' (and everything under it)\n",
      "INFO|2018-11-06T22:51:35|/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/apiclient.py|469| Starting GCS upload to gs://kubeflow-rl-dataflow/cs/staging/preprocess-github-dataset-20181106-225126.1541544688.788675/workflow.tar.gz...\n",
      "INFO|2018-11-06T22:51:35|/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/apiclient.py|484| Completed GCS upload to gs://kubeflow-rl-dataflow/cs/staging/preprocess-github-dataset-20181106-225126.1541544688.788675/workflow.tar.gz\n",
      "INFO|2018-11-06T22:51:35|/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/portability/stager.py|543| Downloading source distribtution of the SDK from PyPi\n",
      "INFO|2018-11-06T22:51:35|/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/portability/stager.py|550| Executing command: ['/home/jovyan/.conda/envs/py2/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpuqyhbz', 'apache-beam==2.6.0', '--no-deps', '--no-binary', ':all:']\n",
      "Collecting apache-beam==2.6.0\n",
      "  Using cached https://files.pythonhosted.org/packages/d6/7d/1294f67c4132e4385edbc6c9efb4f83a7262fa45321e3299c56aaf238de1/apache-beam-2.6.0.zip\n",
      "  Saved /tmp/tmpuqyhbz/apache-beam-2.6.0.zip\n",
      "Successfully downloaded apache-beam\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "INFO|2018-11-06T22:51:37|/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/portability/stager.py|470| Staging SDK sources from PyPI to gs://kubeflow-rl-dataflow/cs/staging/preprocess-github-dataset-20181106-225126.1541544688.788675/dataflow_python_sdk.tar\n",
      "INFO|2018-11-06T22:51:37|/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/apiclient.py|469| Starting GCS upload to gs://kubeflow-rl-dataflow/cs/staging/preprocess-github-dataset-20181106-225126.1541544688.788675/dataflow_python_sdk.tar...\n",
      "INFO|2018-11-06T22:51:37|/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/apiclient.py|484| Completed GCS upload to gs://kubeflow-rl-dataflow/cs/staging/preprocess-github-dataset-20181106-225126.1541544688.788675/dataflow_python_sdk.tar\n",
      "INFO|2018-11-06T22:51:37|/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/portability/stager.py|527| Downloading binary distribtution of the SDK from PyPi\n",
      "INFO|2018-11-06T22:51:37|/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/portability/stager.py|550| Executing command: ['/home/jovyan/.conda/envs/py2/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpuqyhbz', 'apache-beam==2.6.0', '--no-deps', '--only-binary', ':all:', '--python-version', '27', '--implementation', 'cp', '--abi', 'cp27mu', '--platform', 'manylinux1_x86_64']\n",
      "Collecting apache-beam==2.6.0\n",
      "  Using cached https://files.pythonhosted.org/packages/26/44/38642c061f1f1d1ffc166e452f74ec1cc5d14be5afccc0b970af5be34828/apache_beam-2.6.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "  Saved /tmp/tmpuqyhbz/apache_beam-2.6.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Successfully downloaded apache-beam\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "INFO|2018-11-06T22:51:38|/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/portability/stager.py|481| Staging binary distribution of the SDK from PyPI to gs://kubeflow-rl-dataflow/cs/staging/preprocess-github-dataset-20181106-225126.1541544688.788675/apache_beam-2.6.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "INFO|2018-11-06T22:51:38|/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/apiclient.py|469| Starting GCS upload to gs://kubeflow-rl-dataflow/cs/staging/preprocess-github-dataset-20181106-225126.1541544688.788675/apache_beam-2.6.0-cp27-cp27mu-manylinux1_x86_64.whl...\n",
      "INFO|2018-11-06T22:51:39|/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/apiclient.py|484| Completed GCS upload to gs://kubeflow-rl-dataflow/cs/staging/preprocess-github-dataset-20181106-225126.1541544688.788675/apache_beam-2.6.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "INFO|2018-11-06T22:51:40|/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/apiclient.py|559| Create job: <Job\n",
      " createTime: u'2018-11-06T22:51:40.762755Z'\n",
      " currentStateTime: u'1970-01-01T00:00:00Z'\n",
      " id: u'2018-11-06_14_51_39-2093652102174400479'\n",
      " location: u'us-central1'\n",
      " name: u'preprocess-github-dataset-20181106-225126'\n",
      " projectId: u'kubeflow-rl'\n",
      " stageStates: []\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO|2018-11-06T22:51:40|/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/apiclient.py|561| Created job with id: [2018-11-06_14_51_39-2093652102174400479]\n",
      "INFO|2018-11-06T22:51:40|/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/apiclient.py|568| To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobsDetail/locations/us-central1/jobs/2018-11-06_14_51_39-2093652102174400479?project=kubeflow-rl\n",
      "INFO|2018-11-06T22:51:40|/mnt/nfs-east1-d/work/examples/code_search/src/code_search/dataflow/cli/preprocess_github_dataset.py|75| Submitted Dataflow job: <DataflowPipelineResult 2018-11-06_14_51_39-2093652102174400479 PENDING>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "JOB_NAME=\"preprocess-github-dataset-$(date +'%Y%m%d-%H%M%S')\"\n",
    "\n",
    "!/home/jovyan/.conda/envs/py2/bin/python -m code_search.dataflow.cli.preprocess_github_dataset \\\n",
    "        --runner \"DataflowRunner\" \\\n",
    "        --project \"kubeflow-rl\" \\\n",
    "        --target_dataset \"github_function_embeddings\" \\\n",
    "        --data_dir \"gs://kubeflow-rl-dataflow/cs/data\" \\\n",
    "        --job_name \"preprocess-github-dataset-$(date +'%Y%m%d-%H%M%S')\" \\\n",
    "        --temp_location \"gs://kubeflow-rl-dataflow/cs/tmp\" \\\n",
    "        --staging_location \"gs://kubeflow-rl-dataflow/cs/staging\" \\\n",
    "        --worker_machine_type \"n1-standard-64\" \\\n",
    "        --num_workers \"5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's try a tokenization step that retains newline characters as a special symbol that won't interfere with\n",
    "# downstream steps that write examples to a CSV (where a single line denotes a single example). E.g. converting\n",
    "# \\n's to @@@@ (something we should almost never see otherwise). We can then check after the vocab generation\n",
    "# phase whether @@@@ has its own vocab entry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "import ast\n",
    "import astor\n",
    "import nltk.tokenize as tokenize\n",
    "import spacy\n",
    "\n",
    "en = spacy.load('en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foo', 'etc', '@@@', 'something', 'else', '@@@']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def tokenize_code(text):\n",
    "  \"\"\"Tokenize code strings.\n",
    "\n",
    "  This simply considers whitespaces as token delimiters.\n",
    "\n",
    "  Args:\n",
    "    text: A code string to be tokenized.\n",
    "\n",
    "  Returns:\n",
    "    A list of strings representing the tokens in the code.\n",
    "  \"\"\"\n",
    "\n",
    "  tokenization = []\n",
    "  for i, line in enumerate(text.split(\"\\n\")):\n",
    "    tokenization.extend(tokenize.RegexpTokenizer(r'\\w+').tokenize(line))\n",
    "    tokenization.append(\"@@@\")\n",
    "  return tokenization\n",
    "\n",
    "tokenize_code(\"foo \\t etc \\n something else\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hmm this isn't ideal because we lose a lot of the meaning of the code this way...\n",
    "# In python spaces, tabs, and newlines all have important meanings.\n",
    "\n",
    "# Will probably try parsing code into a single line AST string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'hello',\n",
       " u'world',\n",
       " u'args',\n",
       " u':',\n",
       " u'my_arg(str',\n",
       " u')',\n",
       " u':',\n",
       " u'something',\n",
       " u'awesome',\n",
       " u'.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In python2 we need to call decode but in python3 strings\n",
    "# are always unicode.\n",
    "def _maybe_decode(s):\n",
    "  if sys.version_info[0] < 3:\n",
    "    return s.decode(\"utf-8\")\n",
    "  return s\n",
    "\n",
    "def tokenize_docstring(text):\n",
    "  \"\"\"Tokenize docstrings.\n",
    "\n",
    "  Args:\n",
    "    text: A docstring to be tokenized.\n",
    "\n",
    "  Returns:\n",
    "    A list of strings representing the tokens in the docstring.\n",
    "  \"\"\"\n",
    "  tokens = en.tokenizer(_maybe_decode(text))\n",
    "  return [token.text.lower() for token in tokens if not token.is_space]\n",
    "\n",
    "tokenize_docstring(\"  hello world \\n    Args:    my_arg(str): Something awesome. \\n    \")\n",
    "\n",
    "# Not making this change for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# So it looks like the current preproc pipeline is processing all of the available years of data.\n",
    "# So perhaps it would be helpful to consider multiple languages to have more data to consider and at the\n",
    "# same time increase the min star count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/scipy/optimize/_minimize.py:32: ImportWarning: Not importing directory '/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/scipy/optimize/lbfgsb': missing __init__.py\n",
      "  from .lbfgsb import _minimize_lbfgsb\n",
      "/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/scipy/spatial/__init__.py:95: ImportWarning: Not importing directory '/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/scipy/spatial/qhull': missing __init__.py\n",
      "  from .qhull import *\n",
      "INFO|2018-11-14T23:51:14|/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/code_search/dataflow/cli/preprocess_github_dataset.py|58| Reading data using a query.\n",
      "INFO|2018-11-14T23:51:14|/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/code_search/dataflow/transforms/github_dataset.py|64| Writing results to BigQuery kubeflow-rl:github_function_embeddings_alt.token_pairs\n",
      "INFO|2018-11-14T23:51:14|/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/apiclient.py|469| Starting GCS upload to gs://kubeflow-rl-dataflow/cs/staging/preprocess-github-alt-dataset-20181114-235111.1542239474.150747/pipeline.pb...\n",
      "INFO|2018-11-14T23:51:14|/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/oauth2client/transport.py|157| Attempting refresh to obtain initial access_token\n",
      "INFO|2018-11-14T23:51:14|/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/apiclient.py|484| Completed GCS upload to gs://kubeflow-rl-dataflow/cs/staging/preprocess-github-alt-dataset-20181114-235111.1542239474.150747/pipeline.pb\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/.conda/envs/py2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/home/jovyan/.conda/envs/py2/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/code_search/dataflow/cli/preprocess_github_dataset.py\", line 88, in <module>\n",
      "    preprocess_github_dataset()\n",
      "  File \"/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/code_search/dataflow/cli/preprocess_github_dataset.py\", line 74, in preprocess_github_dataset\n",
      "    result = pipeline.run()\n",
      "  File \"/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/pipeline.py\", line 407, in run\n",
      "    return self.runner.run_pipeline(self)\n",
      "  File \"/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/dataflow/dataflow_runner.py\", line 371, in run_pipeline\n",
      "    self.dataflow_client.create_job(self.job), self)\n",
      "  File \"/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/utils/retry.py\", line 184, in wrapper\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/apiclient.py\", line 490, in create_job\n",
      "    self.create_job_description(job)\n",
      "  File \"/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/apiclient.py\", line 519, in create_job_description\n",
      "    resources = self._stage_resources(job.options)\n",
      "  File \"/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/apiclient.py\", line 452, in _stage_resources\n",
      "    staging_location=google_cloud_options.staging_location)\n",
      "  File \"/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/apache_beam/runners/portability/stager.py\", line 175, in stage_job_resources\n",
      "    '--setup_file command line option.' % setup_options.setup_file)\n",
      "RuntimeError: The file /home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/code_search/setup.py cannot be found. It was specified in the --setup_file command line option.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This might work if os.chdir() to one that contains a code_search subdir because otherwise it uses the\n",
    "# directory in site-packages which doesn't include a setup.py file.\n",
    "# In any case it worked by copying the command to the terminal and running from the right path.\n",
    "\n",
    "!/home/jovyan/.conda/envs/py2/bin/python -m code_search.dataflow.cli.preprocess_github_dataset \\\n",
    "        --runner \"DataflowRunner\" \\\n",
    "        --project \"kubeflow-rl\" \\\n",
    "        --target_dataset \"github_function_embeddings_alt\" \\\n",
    "        --data_dir \"gs://kubeflow-rl-dataflow/cs/data_alt\" \\\n",
    "        --job_name \"preprocess-github-alt-dataset-$(date +'%Y%m%d-%H%M%S')\" \\\n",
    "        --temp_location \"gs://kubeflow-rl-dataflow/cs/tmp\" \\\n",
    "        --staging_location \"gs://kubeflow-rl-dataflow/cs/staging\" \\\n",
    "        --worker_machine_type \"n1-standard-64\" \\\n",
    "        --num_workers \"5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AST thing?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import astor\n",
    "\n",
    "def tokenize(blob):\n",
    "  s = ast.dump(ast.parse(blob))\n",
    "  s = s.replace(\"(\", \" ( \").replace(\")\", \" )\").replace(\"=\", \" = \").replace(\"[\", \"[ \").replace(\"]\", \" ]\").replace(\"[  ]\", \"[ ]\").replace(\"',\", \"' ,\").replace(\"(  )\", \"( )\").replace(\"),\", \") ,\").replace(\"'\", \" ' \").replace(\"=  '\", \"= '\").replace(\"'  ,\", \"' ,\")\n",
    "  return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Module ( body = [ FunctionDef ( name = ' iter_node ' , args = arguments ( args = [ Name ( id = ' node ' , ctx = Param ( ) ) , Name ( id = ' name ' , ctx = Param ( ) ) , Name ( id = ' unknown ' , ctx = Param ( ) ) , Name ( id = ' list ' , ctx = Param ( ) ) , Name ( id = ' getattr ' , ctx = Param ( ) ) , Name ( id = ' isinstance ' , ctx = Param ( ) ) , Name ( id = ' enumerate ' , ctx = Param ( ) ) , Name ( id = ' missing ' , ctx = Param ( ) ) ], vararg = None, kwarg = None, defaults = [ Str ( s = '  '  ) , Name ( id = ' None ' , ctx = Load ( ) ) , Name ( id = ' list ' , ctx = Load ( ) ) , Name ( id = ' getattr ' , ctx = Load ( ) ) , Name ( id = ' isinstance ' , ctx = Load ( ) ) , Name ( id = ' enumerate ' , ctx = Load ( ) ) , Name ( id = ' NonExistent ' , ctx = Load ( ) ) ] ) , body = [ Expr ( value = Str ( s = ' Iterates over an object:\\\\n       - If the object has a _fields attribute,\\\\n         it gets attributes in the order of this\\\\n         and returns name, value pairs.\\\\n       - Otherwise, if the object is a list instance,\\\\n         it returns name, value pairs for each item\\\\n         in the list, where the name is passed into\\\\n         this function  ( defaults to blank ).\\\\n       - Can update an unknown set with information about\\\\n         attributes that do not exist in fields.\\\\n     '  ) ) , Assign ( targets = [ Name ( id = ' fields ' , ctx = Store ( ) ) ], value = Call ( func = Name ( id = ' getattr ' , ctx = Load ( ) ) , args = [ Name ( id = ' node ' , ctx = Load ( ) ) , Str ( s = ' _fields '  ) , Name ( id = ' None ' , ctx = Load ( ) ) ], keywords = [ ], starargs = None, kwargs = None ) ) , If ( test = Compare ( left = Name ( id = ' fields ' , ctx = Load ( ) ) , ops = [ IsNot ( ) ], comparators = [ Name ( id = ' None ' , ctx = Load ( ) ) ] ) , body = [ For ( target = Name ( id = ' name ' , ctx = Store ( ) ) , iter = Name ( id = ' fields ' , ctx = Load ( ) ) , body = [ Assign ( targets = [ Name ( id = ' value ' , ctx = Store ( ) ) ], value = Call ( func = Name ( id = ' getattr ' , ctx = Load ( ) ) , args = [ Name ( id = ' node ' , ctx = Load ( ) ) , Name ( id = ' name ' , ctx = Load ( ) ) , Name ( id = ' missing ' , ctx = Load ( ) ) ], keywords = [ ], starargs = None, kwargs = None ) ) , If ( test = Compare ( left = Name ( id = ' value ' , ctx = Load ( ) ) , ops = [ IsNot ( ) ], comparators = [ Name ( id = ' missing ' , ctx = Load ( ) ) ] ) , body = [ Expr ( value = Yield ( value = Tuple ( elts = [ Name ( id = ' value ' , ctx = Load ( ) ) , Name ( id = ' name ' , ctx = Load ( ) ) ], ctx = Load ( ) ) ) ) ], orelse = [ ] ) ], orelse = [ ] ) , If ( test = Compare ( left = Name ( id = ' unknown ' , ctx = Load ( ) ) , ops = [ IsNot ( ) ], comparators = [ Name ( id = ' None ' , ctx = Load ( ) ) ] ) , body = [ Expr ( value = Call ( func = Attribute ( value = Name ( id = ' unknown ' , ctx = Load ( ) ) , attr = ' update ' , ctx = Load ( ) ) , args = [ BinOp ( left = Call ( func = Name ( id = ' set ' , ctx = Load ( ) ) , args = [ Call ( func = Name ( id = ' vars ' , ctx = Load ( ) ) , args = [ Name ( id = ' node ' , ctx = Load ( ) ) ], keywords = [ ], starargs = None, kwargs = None ) ], keywords = [ ], starargs = None, kwargs = None ) , op = Sub ( ) , right = Call ( func = Name ( id = ' set ' , ctx = Load ( ) ) , args = [ Name ( id = ' fields ' , ctx = Load ( ) ) ], keywords = [ ], starargs = None, kwargs = None ) ) ], keywords = [ ], starargs = None, kwargs = None ) ) ], orelse = [ ] ) , Expr ( value = Str ( s = ' something '  ) ) ], orelse = [ If ( test = Call ( func = Name ( id = ' isinstance ' , ctx = Load ( ) ) , args = [ Name ( id = ' node ' , ctx = Load ( ) ) , Name ( id = ' list ' , ctx = Load ( ) ) ], keywords = [ ], starargs = None, kwargs = None ) , body = [ For ( target = Name ( id = ' value ' , ctx = Store ( ) ) , iter = Name ( id = ' node ' , ctx = Load ( ) ) , body = [ Expr ( value = Yield ( value = Tuple ( elts = [ Name ( id = ' value ' , ctx = Load ( ) ) , Name ( id = ' name ' , ctx = Load ( ) ) ], ctx = Load ( ) ) ) ) ], orelse = [ ] ) ], orelse = [ ] ) ] ) ], decorator_list = [ ] ) ] )\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "blob = '''\n",
    "\n",
    "def iter_node(node, name='', unknown=None,\n",
    "              # Runtime optimization\n",
    "              list=list, getattr=getattr, isinstance=isinstance,\n",
    "              enumerate=enumerate, missing=NonExistent):\n",
    "    \"\"\"Iterates over an object:\n",
    "       - If the object has a _fields attribute,\n",
    "         it gets attributes in the order of this\n",
    "         and returns name, value pairs.\n",
    "       - Otherwise, if the object is a list instance,\n",
    "         it returns name, value pairs for each item\n",
    "         in the list, where the name is passed into\n",
    "         this function (defaults to blank).\n",
    "       - Can update an unknown set with information about\n",
    "         attributes that do not exist in fields.\n",
    "    \"\"\"\n",
    "    fields = getattr(node, '_fields', None)\n",
    "    if fields is not None:\n",
    "        for name in fields:\n",
    "            value = getattr(node, name, missing)\n",
    "            if value is not missing:\n",
    "                yield value, name\n",
    "        if unknown is not None:\n",
    "            unknown.update(set(vars(node)) - set(fields))\n",
    "        \"\"\"something\"\"\"\n",
    "    elif isinstance(node, list):\n",
    "        for value in node:\n",
    "            yield value, name\n",
    "\n",
    "'''\n",
    "\n",
    "tokenize(blob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Module ( body = [ FunctionDef ( name = ' strip_tree ' , args = arguments ( args = [ Name ( id = ' node ' , ctx = Param ( ) ) , Name ( id = ' iter_node ' , ctx = Param ( ) ) , Name ( id = ' special ' , ctx = Param ( ) ) , Name ( id = ' list ' , ctx = Param ( ) ) , Name ( id = ' isinstance ' , ctx = Param ( ) ) , Name ( id = ' type ' , ctx = Param ( ) ) , Name ( id = ' len ' , ctx = Param ( ) ) ], vararg = None, kwarg = None, defaults = [ Name ( id = ' iter_node ' , ctx = Load ( ) ) , Attribute ( value = Name ( id = ' ast ' , ctx = Load ( ) ) , attr = ' AST ' , ctx = Load ( ) ) , Name ( id = ' list ' , ctx = Load ( ) ) , Name ( id = ' isinstance ' , ctx = Load ( ) ) , Name ( id = ' type ' , ctx = Load ( ) ) , Name ( id = ' len ' , ctx = Load ( ) ) ] ) , body = [ Expr ( value = Str ( s = ' Strips an AST by removing all attributes not in _fields.\\\\n    Returns a set of the names of all attributes stripped.\\\\n    This canonicalizes two trees for comparison purposes.\\\\n     '  ) ) , Assign ( targets = [ Name ( id = ' stripped ' , ctx = Store ( ) ) ], value = Call ( func = Name ( id = ' set ' , ctx = Load ( ) ) , args = [ ], keywords = [ ], starargs = None, kwargs = None ) ) , FunctionDef ( name = ' strip ' , args = arguments ( args = [ Name ( id = ' node ' , ctx = Param ( ) ) , Name ( id = ' indent ' , ctx = Param ( ) ) ], vararg = None, kwarg = None, defaults = [ ] ) , body = [ Assign ( targets = [ Name ( id = ' unknown ' , ctx = Store ( ) ) ], value = Call ( func = Name ( id = ' set ' , ctx = Load ( ) ) , args = [ ], keywords = [ ], starargs = None, kwargs = None ) ) , Assign ( targets = [ Name ( id = ' leaf ' , ctx = Store ( ) ) ], value = Name ( id = ' True ' , ctx = Load ( ) ) ) , For ( target = Tuple ( elts = [ Name ( id = ' subnode ' , ctx = Store ( ) ) , Name ( id = ' _ ' , ctx = Store ( ) ) ], ctx = Store ( ) ) , iter = Call ( func = Name ( id = ' iter_node ' , ctx = Load ( ) ) , args = [ Name ( id = ' node ' , ctx = Load ( ) ) ], keywords = [ keyword ( arg = ' unknown ' , value = Name ( id = ' unknown ' , ctx = Load ( ) ) ) ], starargs = None, kwargs = None ) , body = [ Assign ( targets = [ Name ( id = ' leaf ' , ctx = Store ( ) ) ], value = Name ( id = ' False ' , ctx = Load ( ) ) ) , Expr ( value = Call ( func = Name ( id = ' strip ' , ctx = Load ( ) ) , args = [ Name ( id = ' subnode ' , ctx = Load ( ) ) , BinOp ( left = Name ( id = ' indent ' , ctx = Load ( ) ) , op = Add ( ) , right = Str ( s = '      '  ) ) ], keywords = [ ], starargs = None, kwargs = None ) ) ], orelse = [ ] ) , If ( test = Name ( id = ' leaf ' , ctx = Load ( ) ) , body = [ If ( test = Call ( func = Name ( id = ' isinstance ' , ctx = Load ( ) ) , args = [ Name ( id = ' node ' , ctx = Load ( ) ) , Name ( id = ' special ' , ctx = Load ( ) ) ], keywords = [ ], starargs = None, kwargs = None ) , body = [ Assign ( targets = [ Name ( id = ' unknown ' , ctx = Store ( ) ) ], value = Call ( func = Name ( id = ' set ' , ctx = Load ( ) ) , args = [ Call ( func = Name ( id = ' vars ' , ctx = Load ( ) ) , args = [ Name ( id = ' node ' , ctx = Load ( ) ) ], keywords = [ ], starargs = None, kwargs = None ) ], keywords = [ ], starargs = None, kwargs = None ) ) ], orelse = [ ] ) ], orelse = [ ] ) , Expr ( value = Call ( func = Attribute ( value = Name ( id = ' stripped ' , ctx = Load ( ) ) , attr = ' update ' , ctx = Load ( ) ) , args = [ Name ( id = ' unknown ' , ctx = Load ( ) ) ], keywords = [ ], starargs = None, kwargs = None ) ) , For ( target = Name ( id = ' name ' , ctx = Store ( ) ) , iter = Name ( id = ' unknown ' , ctx = Load ( ) ) , body = [ Expr ( value = Call ( func = Name ( id = ' delattr ' , ctx = Load ( ) ) , args = [ Name ( id = ' node ' , ctx = Load ( ) ) , Name ( id = ' name ' , ctx = Load ( ) ) ], keywords = [ ], starargs = None, kwargs = None ) ) ], orelse = [ ] ) , If ( test = Call ( func = Name ( id = ' hasattr ' , ctx = Load ( ) ) , args = [ Name ( id = ' node ' , ctx = Load ( ) ) , Str ( s = ' ctx '  ) ], keywords = [ ], starargs = None, kwargs = None ) , body = [ Expr ( value = Call ( func = Name ( id = ' delattr ' , ctx = Load ( ) ) , args = [ Name ( id = ' node ' , ctx = Load ( ) ) , Str ( s = ' ctx '  ) ], keywords = [ ], starargs = None, kwargs = None ) ) , If ( test = Compare ( left = Str ( s = ' ctx '  ) , ops = [ In ( ) ], comparators = [ Attribute ( value = Name ( id = ' node ' , ctx = Load ( ) ) , attr = ' _fields ' , ctx = Load ( ) ) ] ) , body = [ Assign ( targets = [ Name ( id = ' mylist ' , ctx = Store ( ) ) ], value = Call ( func = Name ( id = ' list ' , ctx = Load ( ) ) , args = [ Attribute ( value = Name ( id = ' node ' , ctx = Load ( ) ) , attr = ' _fields ' , ctx = Load ( ) ) ], keywords = [ ], starargs = None, kwargs = None ) ) , Expr ( value = Call ( func = Attribute ( value = Name ( id = ' mylist ' , ctx = Load ( ) ) , attr = ' remove ' , ctx = Load ( ) ) , args = [ Str ( s = ' ctx '  ) ], keywords = [ ], starargs = None, kwargs = None ) ) , Assign ( targets = [ Attribute ( value = Name ( id = ' node ' , ctx = Load ( ) ) , attr = ' _fields ' , ctx = Store ( ) ) ], value = Name ( id = ' mylist ' , ctx = Load ( ) ) ) ], orelse = [ ] ) ], orelse = [ ] ) ], decorator_list = [ ] ) , Expr ( value = Call ( func = Name ( id = ' strip ' , ctx = Load ( ) ) , args = [ Name ( id = ' node ' , ctx = Load ( ) ) , Str ( s = '  '  ) ], keywords = [ ], starargs = None, kwargs = None ) ) , Return ( value = Name ( id = ' stripped ' , ctx = Load ( ) ) ) ], decorator_list = [ ] ) ] )\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = '''\n",
    "\n",
    "def strip_tree(node,\n",
    "               # Runtime optimization\n",
    "               iter_node=iter_node, special=ast.AST,\n",
    "               list=list, isinstance=isinstance, type=type, len=len):\n",
    "    \"\"\"Strips an AST by removing all attributes not in _fields.\n",
    "    Returns a set of the names of all attributes stripped.\n",
    "    This canonicalizes two trees for comparison purposes.\n",
    "    \"\"\"\n",
    "    stripped = set()\n",
    "\n",
    "    def strip(node, indent):\n",
    "        unknown = set()\n",
    "        leaf = True\n",
    "        for subnode, _ in iter_node(node, unknown=unknown):\n",
    "            leaf = False\n",
    "            strip(subnode, indent + '    ')\n",
    "        if leaf:\n",
    "            if isinstance(node, special):\n",
    "                unknown = set(vars(node))\n",
    "        stripped.update(unknown)\n",
    "        for name in unknown:\n",
    "            delattr(node, name)\n",
    "        if hasattr(node, 'ctx'):\n",
    "            delattr(node, 'ctx')\n",
    "            if 'ctx' in node._fields:\n",
    "                mylist = list(node._fields)\n",
    "                mylist.remove('ctx')\n",
    "                node._fields = mylist\n",
    "    strip(node, '')\n",
    "    return stripped\n",
    "\n",
    "'''\n",
    "\n",
    "tokenize(blob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Module ( body = [ FunctionDef ( name = ' hello_world ' , args = arguments ( args = [ ], vararg = None, kwarg = None, defaults = [ ] ) , body = [ Print ( dest = None, values = [ Str ( s = ' hello world '  ) ], nl = True ) ], decorator_list = [ ] ) ] )\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = '''\n",
    "def hello_world():\n",
    "  print \"hello world\"\n",
    "'''\n",
    "\n",
    "tokenize(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
