{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS Local search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "\n",
    "1. Export model\n",
    "2. Run large-batch inference of code embeddings, write to CSV.\n",
    "3. Load code embeddings into nmslib search index.\n",
    "4. Using tf.Eager, perform inference for a query in notebook.\n",
    "5. Search that query embedding against the search index, listing hits in notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use tf.Eager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be far slower in part because for now I'm not able to use batch sizes > 1 (for what ever reason). But - I know this will for sure work. And it will probably take less than like 5min to compute embeddings for like 10k examples.... 200 takes about 10-20s... ok so more like 15min.... an index of 3k examples in 5min... so maybe could build an index using a set of things that are or aren't known to go together, e.g. two very distinct codebases instead of a random sampling of all... maybe just use the t2t library?..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "import functools\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.layers import common_layers\n",
    "from tensor2tensor.models import transformer\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.utils import t2t_model\n",
    "from tensor2tensor.models.transformer import Transformer\n",
    "\n",
    "from tensor2tensor.models.transformer import transformer_base\n",
    "\n",
    "from tensorflow.contrib.eager.python import tfe\n",
    "tfe.enable_eager_execution()\n",
    "Modes = tf.estimator.ModeKeys\n",
    "\n",
    "\n",
    "from tk.models import similarity_transformer\n",
    "from tk.data_generators import function_docstring\n",
    "\n",
    "import numpy as np; np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting T2TModel mode to 'infer'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-10-29 17:22:53,757] Setting T2TModel mode to 'infer'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-10-29 17:22:53,761] Setting hparams.layer_prepostprocess_dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-10-29 17:22:53,763] Setting hparams.symbol_dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-10-29 17:22:53,767] Setting hparams.attention_dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-10-29 17:22:53,770] Setting hparams.dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-10-29 17:22:53,772] Setting hparams.relu_dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading data files from /mnt/nfs-east1-d/data/github_function_docstring-dev*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-10-29 17:22:53,839] Reading data files from /mnt/nfs-east1-d/data/github_function_docstring-dev*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:partition: 0 num_data_files: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-10-29 17:22:53,846] partition: 0 num_data_files: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mp_constrained_embedding = function_docstring.GithubConstrainedEmbedding()\n",
    "\n",
    "data_dir = \"/mnt/nfs-east1-d/data\"\n",
    "\n",
    "hparams = similarity_transformer.similarity_transformer_tiny()\n",
    "hparams.data_dir = data_dir\n",
    "\n",
    "p_hparams = mp_constrained_embedding.get_hparams(hparams)\n",
    "\n",
    "model = similarity_transformer.ConstrainedEmbeddingTransformer(\n",
    "    hparams, tf.estimator.ModeKeys.PREDICT, p_hparams\n",
    ")\n",
    "\n",
    "# Get the encoders from the problem\n",
    "encoders = mp_constrained_embedding.feature_encoders(data_dir)\n",
    "\n",
    "# Setup helper functions for encoding and decoding\n",
    "def encode(input_str, output_str=None):\n",
    "  \"\"\"Input str to features dict, ready for inference\"\"\"\n",
    "  inputs = encoders[\"inputs\"].encode(input_str) + [1]  # add EOS id\n",
    "  batch_inputs = tf.reshape(inputs, [1, -1, 1])  # Make it 3D.\n",
    "  return {\"inputs\": batch_inputs}\n",
    "\n",
    "def decode(integers):\n",
    "  \"\"\"List of ints to str\n",
    "  \n",
    "  For decoding an integer encoding to its string representation,\n",
    "  not for decoding an embedding vector into the same.\n",
    "  \"\"\"\n",
    "  integers = list(np.squeeze(integers))\n",
    "  if 1 in integers:\n",
    "    integers = integers[:integers.index(1)]\n",
    "  return encoders[\"inputs\"].decode(np.squeeze(integers))\n",
    "\n",
    "batch_size = 1\n",
    "train_dataset = mp_constrained_embedding.dataset(Modes.PREDICT, data_dir)\n",
    "train_dataset = train_dataset.repeat(None).batch(batch_size)\n",
    "\n",
    "iterator = tfe.Iterator(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed_many(dataset_iterator, model, ckpt_path, num=100):\n",
    "    embeddings = []\n",
    "    num_examples = 100\n",
    "\n",
    "    with tfe.restore_variables_on_create(ckpt_path):\n",
    "\n",
    "      for i in range(0, num):\n",
    "        example = dataset_iterator.next()\n",
    "\n",
    "        #doc_emb, _ = model({\"inputs\": example[\"docstring\"]})\n",
    "        code_emb, _ = model({\"inputs\": example[\"code\"]})\n",
    "\n",
    "        #embeddings.append([doc_emb, code_emb, decode(example[\"docstring\"]), decode(example[\"code\"])])\n",
    "        embeddings.append([code_emb, decode(example[\"code\"]), decode(example[\"docstring\"])])\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "          print(\"Processing step %s\" % i)\n",
    "    \n",
    "    print(\"Finished embedding %s\" % num)\n",
    "\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"gs://kubeflow-rl-checkpoints/comparisons/cs-v7-lvslicenet-ts1-tcs1-tm1-exl1-ntm1/cs-v7-lvslicenet-ts1-tcs1-tm1-exl1-ntm1-j1026-1730-f4d7/output/model.ckpt-215387\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-29 17:22:53.954837\n"
     ]
    }
   ],
   "source": [
    "print datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing step 0\n",
      "Processing step 100\n",
      "Processing step 200\n",
      "Processing step 300\n",
      "Processing step 400\n",
      "Processing step 500\n",
      "Processing step 600\n",
      "Processing step 700\n",
      "Processing step 800\n",
      "Processing step 900\n",
      "Processing step 1000\n",
      "Processing step 1100\n",
      "Processing step 1200\n",
      "Processing step 1300\n",
      "Processing step 1400\n",
      "Processing step 1500\n",
      "Processing step 1600\n",
      "Processing step 1700\n",
      "Processing step 1800\n",
      "Processing step 1900\n",
      "Processing step 2000\n",
      "Processing step 2100\n",
      "Processing step 2200\n",
      "Processing step 2300\n",
      "Processing step 2400\n",
      "Processing step 2500\n",
      "Processing step 2600\n",
      "Processing step 2700\n",
      "Processing step 2800\n",
      "Processing step 2900\n",
      "Processing step 3000\n",
      "Processing step 3100\n",
      "Processing step 3200\n",
      "Processing step 3300\n",
      "Processing step 3400\n",
      "Processing step 3500\n",
      "Processing step 3600\n",
      "Processing step 3700\n",
      "Processing step 3800\n",
      "Processing step 3900\n",
      "Processing step 4000\n",
      "Processing step 4100\n",
      "Processing step 4200\n",
      "Processing step 4300\n",
      "Processing step 4400\n",
      "Processing step 4500\n",
      "Processing step 4600\n",
      "Processing step 4700\n",
      "Processing step 4800\n",
      "Processing step 4900\n",
      "Processing step 5000\n",
      "Processing step 5100\n",
      "Processing step 5200\n",
      "Processing step 5300\n",
      "Processing step 5400\n",
      "Processing step 5500\n",
      "Processing step 5600\n",
      "Processing step 5700\n",
      "Processing step 5800\n",
      "Processing step 5900\n",
      "Processing step 6000\n",
      "Processing step 6100\n",
      "Processing step 6200\n",
      "Processing step 6300\n",
      "Processing step 6400\n",
      "Processing step 6500\n",
      "Processing step 6600\n",
      "Processing step 6700\n",
      "Processing step 6800\n",
      "Processing step 6900\n",
      "Processing step 7000\n",
      "Processing step 7100\n",
      "Processing step 7200\n",
      "Processing step 7300\n",
      "Processing step 7400\n",
      "Processing step 7500\n",
      "Processing step 7600\n",
      "Processing step 7700\n",
      "Processing step 7800\n",
      "Processing step 7900\n",
      "Processing step 8000\n",
      "Processing step 8100\n",
      "Processing step 8200\n",
      "Processing step 8300\n",
      "Processing step 8400\n",
      "Processing step 8500\n",
      "Processing step 8600\n",
      "Processing step 8700\n",
      "Processing step 8800\n",
      "Processing step 8900\n",
      "Processing step 9000\n",
      "Processing step 9100\n",
      "Processing step 9200\n",
      "Processing step 9300\n",
      "Processing step 9400\n",
      "Processing step 9500\n",
      "Processing step 9600\n",
      "Processing step 9700\n",
      "Processing step 9800\n",
      "Processing step 9900\n",
      "Finished embedding 10000\n"
     ]
    }
   ],
   "source": [
    "embeddings = embed_many(iterator, model, ckpt_path, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-29 17:35:52.532747\n"
     ]
    }
   ],
   "source": [
    "print datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numpy_embedding_vectors = np.asarray([thing[0].numpy()[0] for thing in embeddings])\n",
    "\n",
    "embedding_data_path = \"/mnt/nfs-east1-d/tmp/embeddings.csv\"\n",
    "\n",
    "np.savetxt(embedding_data_path, np.asarray([thing[0].numpy()[0] for thing in embeddings]), delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nmslib\n",
    "import pprint\n",
    "\n",
    "del index\n",
    "\n",
    "def build_index(index_save_path, index_data):\n",
    "  index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "  index.addDataPointBatch(index_data)\n",
    "  index.createIndex({'post': 2}, print_progress=True)\n",
    "  index.saveIndex(index_save_path)\n",
    "  return index\n",
    "\n",
    "def embed_query(model, ckpt_path, query):\n",
    "  with tfe.restore_variables_on_create(ckpt_path):\n",
    "    return model(encode(query))[0].numpy()[0]\n",
    "\n",
    "ckpt_path = \"gs://kubeflow-rl-checkpoints/comparisons/cs-v7-lvslicenet-ts1-tcs1-tm1-exl1-ntm1/cs-v7-lvslicenet-ts1-tcs1-tm1-exl1-ntm1-j1026-1730-f4d7/output/model.ckpt-215387\"\n",
    "query_fn = functools.partial(embed_query, model, ckpt_path)\n",
    "\n",
    "local_index_save_path = \"/mnt/nfs-east1-d/tmp/index-004\"\n",
    "\n",
    "k = 10000\n",
    "\n",
    "def results_for_query(query):\n",
    "  query_embedding = query_fn(query)\n",
    "\n",
    "  idxs, dists = index.knnQuery(query_embedding, k=k)\n",
    "\n",
    "  hits = []\n",
    "    \n",
    "  for (i, d) in zip(idxs, dists):\n",
    "    hits.append((embeddings[i][2], embeddings[i][1], d))\n",
    "  return hits\n",
    "\n",
    "def print_hits(hit_subset):\n",
    "  for hit in hit_subset:\n",
    "    print(hit[0])\n",
    "    print(hit[1])\n",
    "    print(hit[2])\n",
    "    print(\"\\n\")\n",
    "  print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index = build_index(local_index_save_path, numpy_embedding_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hits = results_for_query(\"fetch the pagination marker field from flask.request.args\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when processing an error , consume the response body to ensure it is n't mixed up with the next request in case the connection is kept alive .\n",
      "staticmethod def _drain response try response read except socket error pass\n",
      "0.0267815\n",
      "\n",
      "\n",
      "scrapes the list of modules associated with circuitpython . causes scrapy to follow the links to the module docs and uses a different parser to extract the api information contained therein .\n",
      "def parse self response for next_page in response css div toctree wrapper li a yield response follow next_page self parse_api\n",
      "0.027252793\n",
      "\n",
      "\n",
      "redirect to the detail view after updating the registration\n",
      "def test_redirect_to_pet self resp self client get reverse meupet update_register args self pet request_key self assertRedirects resp reverse meupet detail args self pet slug\n",
      "0.028014004\n",
      "\n",
      "\n",
      "unpin the database from master in the current db .\n",
      "pytest fixture autouse True def unpin_db request request addfinalizer pinning unpin_this_thread\n",
      "0.028688014\n",
      "\n",
      "\n",
      "iterate over accounts this account is subscribed to .\n",
      "def watching self return self _match_subscription both to\n",
      "0.030877292\n",
      "\n",
      "\n",
      "--\n",
      "todo test if a formula is closed\n",
      "def is_closed self pass\n",
      "0.055523396\n",
      "\n",
      "\n",
      "calculates a checksum with the provided algorithm .\n",
      "def has_valid_checksum self number multiple_table 6 5 7 2 3 4 5 6 7 result 0 for i in range len number 1 result int number i multiple_table i result 11 if result int number 1 return True else return False\n",
      "0.057335675\n",
      "\n",
      "\n",
      "reshaping happens during the call to forward .\n",
      "def reshape self bottom top pass\n",
      "0.058642924\n",
      "\n",
      "\n",
      "calculate the mean nearest - cluster distance for sample i.\n",
      "def _nearest_cluster_distance distances_row labels i label labels i b np min np mean distances_row labels cur_label for cur_label in set labels if not cur_label label return b\n",
      "0.05865109\n",
      "\n",
      "\n",
      "declaration : message | include | type | typedef | using\n",
      "def p_declaration p p 0 p 1\n",
      "0.067786455\n",
      "\n",
      "\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Most similar\n",
    "print_hits(hits[0:5])\n",
    "\n",
    "# Least similar\n",
    "print_hits(hits[-5:])\n",
    "\n",
    "# Nope.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "required by ` flask - login < https://flask - login.readthedocs.org / en / latest/>`_.\n",
      "fetch the pagination marker field from flask.request.args .\n",
      "yield a tuple for each flask handler containing annotated methods .\n",
      "validate auth0 tokens passed in the request 's header , hence ensuring that the user is authenticated . code copied from : https://github.com/auth0/auth0-python/tree/master/examples/flask-api\n",
      "create a flask application and load all configuration files\n"
     ]
    }
   ],
   "source": [
    "for etuple in embeddings:\n",
    "  if \"lask\" in etuple[2]:\n",
    "    print etuple[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# That exact doc string is present in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fetch the pagination marker field from flask.request.args .', 'request_field marker raises_coercion_exceptions def marker_field value assert uuidutils is_uuid_like value _ Marker not UUID like return value', 0.04419422)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for etuple in hits:\n",
    "  if \"lask\" in etuple[0]:\n",
    "    print etuple\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.041648388"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.median([hit[2] for hit in hits])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5970c9b7246b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mhit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Not a highly ranked hit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hits' is not defined"
     ]
    }
   ],
   "source": [
    "[hit[2] for hit in hits]\n",
    "\n",
    "# Not a highly ranked hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
