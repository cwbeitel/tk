{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import logging\n",
    "import numpy\n",
    "import pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tensor2tensor.utils import trainer_lib\n",
    "from tensor2tensor import problems\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.data_generators import allen_brain\n",
    "from tensor2tensor.data_generators import allen_brain_utils\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.eager.python import tfe\n",
    "tfe.enable_eager_execution()\n",
    "Modes = tf.estimator.ModeKeys\n",
    "\n",
    "from tk.util import hack_dict_to_cli_args\n",
    "from tk import experiment\n",
    "from tk import util\n",
    "\n",
    "from tensorboard.backend.event_processing import event_file_loader\n",
    "from protobuf_to_dict import protobuf_to_dict\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _stage(local_app_root, remote_app_root):\n",
    "    \n",
    "    if not os.path.exists(local_app_root):\n",
    "        raise ValueError(\"Can't stage from a non-existent source, \"\n",
    "                         \"saw %s\" % local_app_root)\n",
    "\n",
    "    shutil.copytree(local_app_root, remote_app_root)\n",
    "\n",
    "\n",
    "def _configure_experiment(base_name, num_gpu_per_worker=1,\n",
    "                          problem=\"img2img_allen_brain_dim8to32\",\n",
    "                          model=\"img2img_transformer\",\n",
    "                          hparams_set=\"img2img_transformer2d_tiny\",\n",
    "                          batch_size=4,\n",
    "                          num_steps=100000,\n",
    "                          num_workers=0,\n",
    "                          num_ps=0,\n",
    "                          ps_gpu=1):\n",
    "\n",
    "    app_root = \"/mnt/nfs-east1-d/work/tk\"\n",
    "    \n",
    "    output_dir = os.path.join(app_root, \"output\")\n",
    "\n",
    "    job_name = util.generate_job_name(base_name)\n",
    "\n",
    "    train_args = {\n",
    "        \"problem\": problem,\n",
    "        \"model\": model,\n",
    "        \"hparams_set\": hparams_set,\n",
    "        \"data_dir\": \"/mnt/nfs-east1-d/data\",\n",
    "        \"output_dir\": output_dir,\n",
    "        \"train_steps\": num_steps,\n",
    "        \"schedule\": \"train\",\n",
    "        \"profile\": False,\n",
    "        \"log_device_placement\": False,\n",
    "        \"worker_gpu\": num_gpu_per_worker,\n",
    "        \"ps_gpu\": ps_gpu,\n",
    "        \"save_checkpoints_secs\": 1800,\n",
    "        \"dbgprofile\": False, # Saves profiling timelines, viewable in chrome://tracing\n",
    "        \"ssd_mount_path\": \"/mnt/disks/ssd0\",\n",
    "        \"worker_gpu_memory_fraction\": 0.95,\n",
    "        \"hparams\": \"'batch_size=%s'\" % batch_size\n",
    "    }\n",
    "\n",
    "    args = {\n",
    "        \"job_name\": job_name,\n",
    "        \"volume_claim_id\": \"nfs-east1-d\",\n",
    "        \"app_root\": app_root,\n",
    "        \"gcp_project\": \"foo\",\n",
    "        \"namespace\": \"kubeflow\",\n",
    "        \"image\": \"tensorflow/tensorflow:latest-gpu\",\n",
    "        \"smoke\": True,\n",
    "        \"batch\": False,\n",
    "        \"train_args\": train_args,\n",
    "        \"cpu\": 7,\n",
    "        \"memory\": \"40Gi\",\n",
    "        \"num_gpu\": num_gpu_per_worker,\n",
    "        \n",
    "        # DEV\n",
    "        \"master_gpu\": num_gpu_per_worker,\n",
    "        \"ps_gpu\": ps_gpu,\n",
    "        \"worker_gpu\": num_gpu_per_worker,\n",
    "        # --\n",
    "\n",
    "        \"num_local_ssd\": 1,\n",
    "        \"no_wait\": True,\n",
    "        \"num_worker_replicas\": num_workers,\n",
    "        \"num_ps_replicas\": num_ps,\n",
    "        \"selector_labels\": {\n",
    "          \"cloud.google.com/gke-nodepool\": \"train-gpu-preemptible-%sx-hm\" % num_gpu_per_worker,\n",
    "          \"cloud.google.com/gke-accelerator\": \"nvidia-tesla-k80\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    local_app_root = args[\"app_root\"]\n",
    "\n",
    "    testing_storage_base = \"/mnt/nfs-east1-d/comparisons/%s\" % base_name\n",
    "    \n",
    "    remote_app_root = \"%s/%s\" % (testing_storage_base,\n",
    "                                 args[\"job_name\"])\n",
    "\n",
    "    #args[\"train_args\"][\"output_dir\"] = os.path.join(remote_app_root,\n",
    "    #                                                \"output\")\n",
    "    \n",
    "    output_dir_root = \"gs://kubeflow-rl-checkpoints/comparisons/%s\" % base_name\n",
    "    args[\"train_args\"][\"output_dir\"] = os.path.join(output_dir_root,\n",
    "                                                    args[\"job_name\"])\n",
    "    \n",
    "    for job_type in [\"master\", \"ps\"]:\n",
    "        \n",
    "        with open(os.path.join(local_app_root, \"%s-job.sh\" % job_type), \"w\") as f:\n",
    "          f.write(\"ls /mnt\\n\")\n",
    "          f.write(\"cp -r /mnt/nfs-east1-d/data/* /mnt/ssd0/\\n\")\n",
    "          f.write(\"pip install -e %s/vendor/tensor2tensor\\n\" % remote_app_root)\n",
    "          f.write(\"pip install -e %s\\n\" % remote_app_root)\n",
    "          f.write(\"nvidia-smi\\n\")\n",
    "          f.write(\"python -c 'from tensorflow.python.client import device_lib; print(device_lib.list_local_devices())'\\n\")\n",
    "          f.write(\"echo ${TF_CONFIG}\\n\")\n",
    "          f.write(\"cd %s\\n\" % remote_app_root)\n",
    "          cmd = [\"python\", \"-m\", \"tk.experiment\"]\n",
    "        \n",
    "          # Can this be accomplished by tk.experiment, detecting task_type from TF_CONFIG?\n",
    "          #if job_type == \"master\":\n",
    "          #  args[\"train_args\"][\"schedule\"] = \"train\"\n",
    "          #elif job_type == \"ps\":\n",
    "          #  args[\"train_args\"][\"schedule\"] = \"run_std_server\"\n",
    "          # --\n",
    "\n",
    "          cmd.extend(hack_dict_to_cli_args(args[\"train_args\"]))\n",
    "          f.write(\" \".join(cmd) + \"\\n\")\n",
    "          f.write(\"nvidia-smi\\n\")\n",
    "          logging.info(local_app_root)\n",
    "    \n",
    "    _stage(local_app_root, remote_app_root)\n",
    "    args[\"app_root\"] = remote_app_root\n",
    "    args[\"batch\"] = True\n",
    "\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-08-23 20:29:08,444] /mnt/nfs-east1-d/work/tk\n",
      "[2018-08-23 20:29:08,454] /mnt/nfs-east1-d/work/tk\n",
      "[2018-08-23 20:30:36,732] smoke: True\n"
     ]
    }
   ],
   "source": [
    "bsize = 1\n",
    "\n",
    "problem_name = \"img2img_allen_brain_dim8to32\"\n",
    "\n",
    "args = _configure_experiment(\"dist-gcs-shared-3\",\n",
    "                             problem=problem_name,\n",
    "                             num_gpu_per_worker=1,\n",
    "                             batch_size=bsize,\n",
    "                             num_steps=1000,\n",
    "                             num_workers=0,\n",
    "                             num_ps=2)\n",
    "\n",
    "job = experiment.T2TExperiment(**args)\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with more GPUs per worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-08-30 16:46:43,486] /mnt/nfs-east1-d/work/tk\n",
      "[2018-08-30 16:46:43,520] /mnt/nfs-east1-d/work/tk\n",
      "[2018-08-30 16:48:44,419] smoke: True\n"
     ]
    }
   ],
   "source": [
    "bsize = 1\n",
    "\n",
    "problem_name = \"img2img_allen_brain_dim8to32\"\n",
    "\n",
    "args = _configure_experiment(\"dist-b%s\" % bsize,\n",
    "                             problem=problem_name,\n",
    "                             num_gpu_per_worker=1,\n",
    "                             batch_size=bsize,\n",
    "                             num_steps=10000,\n",
    "                             num_workers=0,\n",
    "                             num_ps=2,\n",
    "                             ps_gpu=1)\n",
    "\n",
    "job = experiment.T2TExperiment(**args)\n",
    "\n",
    "job.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
