{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS CE problem v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Autoencoding problems\n",
    "\n",
    "* Crossover problems\n",
    "\n",
    "* Encoder and decoder scoping by modality (either input or target)\n",
    "\n",
    "* Dense layer to map code tensors into 128-dimensional vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.layers import common_layers\n",
    "from tensor2tensor.models import transformer\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.utils import t2t_model\n",
    "\n",
    "from tensor2tensor.models import transformer\n",
    "\n",
    "from tensor2tensor.models.transformer import transformer_base\n",
    "\n",
    "from tensorflow.contrib.eager.python import tfe\n",
    "tfe.enable_eager_execution()\n",
    "Modes = tf.estimator.ModeKeys\n",
    "\n",
    "\n",
    "from tk.models import similarity_transformer\n",
    "from tk.data_generators import function_docstring\n",
    "\n",
    "import numpy as np; np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scoped_flexible_encode(self, inputs, target_space, hparams, scope_name,\n",
    "                           features_key, features=None, losses=None):\n",
    "\n",
    "    with tf.variable_scope(scope_name):\n",
    "\n",
    "        inputs = common_layers.flatten4d3d(inputs)\n",
    "\n",
    "        encoder_input, self_attention_bias, encoder_decoder_attention_bias = (\n",
    "            transformer_prepare_encoder(\n",
    "                inputs, target_space, hparams, features=features))\n",
    "\n",
    "        mlperf_log.transformer_print(\n",
    "            key=mlperf_log.MODEL_HP_LAYER_POSTPROCESS_DROPOUT,\n",
    "            value=hparams.layer_prepostprocess_dropout)\n",
    "\n",
    "        encoder_input = tf.nn.dropout(encoder_input,\n",
    "                                      1.0 - hparams.layer_prepostprocess_dropout)\n",
    "\n",
    "        encoder_output = transformer_encoder(\n",
    "            encoder_input,\n",
    "            self_attention_bias,\n",
    "            hparams,\n",
    "            nonpadding=features_to_nonpadding(features, features_key),\n",
    "            save_weights_to=self.attention_weights,\n",
    "            make_image_summary=not common_layers.is_xla_compiled(),\n",
    "            losses=losses)\n",
    "\n",
    "        return encoder_output, encoder_decoder_attention_bias\n",
    "\n",
    "\n",
    "@registry.register_model\n",
    "class ConstrainedEmbeddingTransformerV2(transformer.Transformer):\n",
    "\n",
    "  def scoped_encoder(self, tensor, scope_name, features,\n",
    "                     target_space=problem.SpaceID.EN_TOK):\n",
    "    hparams = self._hparams\n",
    "    with tf.variable_scope(scope_name):\n",
    "      return self.encode(tensor, target_space, hparams,\n",
    "                         features=features)\n",
    "\n",
    "  def scoped_decoder(self, encoder_output, scope_name, features, encoder_decoder_attention_bias):\n",
    "    \n",
    "    hparams = self._hparams\n",
    "    targets = features[\"targets\"]\n",
    "    targets_shape = common_layers.shape_list(targets)\n",
    "    targets = common_layers.flatten4d3d(targets)\n",
    "    \n",
    "    losses=None #?\n",
    "    \n",
    "    with tf.variable_scope(scope_name):\n",
    "\n",
    "      decoder_input, decoder_self_attention_bias = transformer.transformer_prepare_decoder(\n",
    "        targets, hparams, features=features)\n",
    "\n",
    "      decoder_output = self.decode(\n",
    "        decoder_input,\n",
    "        encoder_output,\n",
    "        encoder_decoder_attention_bias,\n",
    "        decoder_self_attention_bias,\n",
    "        hparams,\n",
    "        nonpadding=transformer.features_to_nonpadding(features, \"targets\"),\n",
    "        losses=losses)\n",
    "\n",
    "    ret = tf.reshape(decoder_output, targets_shape)\n",
    "    \n",
    "    return ret\n",
    "\n",
    "  def string_encoder(self, tensor, features):\n",
    "    return self.scoped_encoder(tensor, \"string_encoder\", features)\n",
    "\n",
    "  def code_encoder(self, tensor, features):\n",
    "    return self.scoped_encoder(tensor, \"code_encoder\", features)\n",
    "\n",
    "  def string_decoder(self, tensor, features, attn_bias):\n",
    "    return self.scoped_decoder(tensor, \"string_decoder\", features, attn_bias)\n",
    "\n",
    "  def code_decoder(self, tensor, features, attn_bias):\n",
    "    return self.scoped_decoder(tensor, \"code_decoder\", features, attn_bias)\n",
    "\n",
    "  def maybe_predict(self, features):\n",
    "    if self._hparams.mode == tf.estimator.ModeKeys.PREDICT:\n",
    "      if self._hparams.predict_mode == \"code\":\n",
    "        code_emb_raw, _ = self.code_encoder(features[\"inputs\"], features)\n",
    "        return self.scoped_dense(code_emb_raw, \"dense_code\"), {\"training\": 0.0} # Not necessary?\n",
    "      elif self._hparams.predict_mode == \"docstring\":\n",
    "        string_emb_raw, _ = self.string_encoder(features[\"inputs\"], features)\n",
    "        return self.scoped_dense(string_emb_raw, \"dense_string\"), {\"training\": 0.0}\n",
    "      else:\n",
    "        return None\n",
    "\n",
    "  def scoped_dense(self, emb, scope_name):\n",
    "    with tf.variable_scope(scope_name):\n",
    "      return tf.nn.dense(emb)\n",
    "\n",
    "  def body(self, features):\n",
    "    \"\"\"Perhaps overly complicated \"constrained embedding cross-/auto-encoder\"......\n",
    "\n",
    "    - Expects features {\"inputs\": doc string, \"targets\": code}\n",
    "    - Computes encoder/decoder losses (d->c, c->d, c->c, d->c)\n",
    "    - Computes embedding similarity loss (e(d)~e(c)?)\n",
    "    - During training, doesn't return a tensor result\n",
    "    - At inference time, returns either code or string embedding\n",
    "      (reduced to single vector) depending on hparams.predict_input_modality.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction = self.maybe_predict(features)\n",
    "    if prediction is not None:\n",
    "      return prediction\n",
    "\n",
    "    losses = {}\n",
    "\n",
    "    string_emb_raw, string_attn_bias = self.string_encoder(features[\"inputs\"], features)\n",
    "    code_emb_raw, code_attn_bias = self.code_encoder(features[\"targets\"], features)\n",
    "    \n",
    "    # Compute the auto- and cross-mappings\n",
    "    code_from_string = self.code_decoder(string_emb_raw, features, string_attn_bias)\n",
    "    \n",
    "    \n",
    "    #string_from_string = self.string_decoder(string_emb_raw, string_attn_bias)\n",
    "    #code_from_code = self.code_decoder(code_emb_raw, code_attn_bias)\n",
    "    #string_from_code = self.string_decoder(code_emb_raw, code_attn_bias)\n",
    "    \n",
    "    cfs = self.top(code_from_string, features)\n",
    "    cfs = self.loss(cfs, features)\n",
    "    \n",
    "    return code_from_string\n",
    "    \n",
    "    #cfs = self.loss(cfs, features)\n",
    "    \n",
    "    #losses.update({\"code_from_string\": cfs})\n",
    "    \n",
    "    \"\"\"\n",
    "    # Compute the embedding similarity loss\n",
    "    losses.update(compute_similarity_costs(\n",
    "        self.reduce_string_emb(string_emb_raw),\n",
    "        self.reduce_code_emb(code_emb_raw),\n",
    "        self._hparams))\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # HACK\n",
    "    losses[\"training\"] = sum(losses.values())\n",
    "\n",
    "    return None, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading data files from /mnt/nfs-east1-d/data/github_function_docstring-dev*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-10-29 22:22:47,074] Reading data files from /mnt/nfs-east1-d/data/github_function_docstring-dev*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:partition: 0 num_data_files: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-10-29 22:22:47,081] partition: 0 num_data_files: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mp_constrained_embedding = function_docstring.GithubConstrainedEmbedding()\n",
    "\n",
    "data_dir = \"/mnt/nfs-east1-d/data\"\n",
    "\n",
    "hparams = similarity_transformer.similarity_transformer_tiny()\n",
    "hparams.data_dir = data_dir\n",
    "\n",
    "p_hparams = mp_constrained_embedding.get_hparams(hparams)\n",
    "\n",
    "model = ConstrainedEmbeddingTransformerV2(\n",
    "    hparams, tf.estimator.ModeKeys.TRAIN, p_hparams\n",
    ")\n",
    "\n",
    "# Get the encoders from the problem\n",
    "encoders = mp_constrained_embedding.feature_encoders(data_dir)\n",
    "\n",
    "# Setup helper functions for encoding and decoding\n",
    "def encode(input_str, output_str=None):\n",
    "  \"\"\"Input str to features dict, ready for inference\"\"\"\n",
    "  inputs = encoders[\"inputs\"].encode(input_str) + [1]  # add EOS id\n",
    "  batch_inputs = tf.reshape(inputs, [1, -1, 1])  # Make it 3D.\n",
    "  return {\"inputs\": batch_inputs}\n",
    "\n",
    "def decode(integers):\n",
    "  \"\"\"List of ints to str\n",
    "  \n",
    "  For decoding an integer encoding to its string representation,\n",
    "  not for decoding an embedding vector into the same.\n",
    "  \"\"\"\n",
    "  integers = list(np.squeeze(integers))\n",
    "  if 1 in integers:\n",
    "    integers = integers[:integers.index(1)]\n",
    "  return encoders[\"inputs\"].decode(np.squeeze(integers))\n",
    "\n",
    "batch_size = 1\n",
    "train_dataset = mp_constrained_embedding.dataset(Modes.PREDICT, data_dir)\n",
    "train_dataset = train_dataset.repeat(None).batch(batch_size)\n",
    "\n",
    "iterator = tfe.Iterator(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Input to reshape is a tensor with 271722 values, but the requested shape has 34780416 [Op:Reshape] name: padded_cross_entropy_size_check",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-8b7ae87d3699>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/tensorflow/python/eager/backprop.pyc\u001b[0m in \u001b[0;36mgrad_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0mthis_tape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_new_tape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m       \u001b[0mend_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mend_node\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         raise ValueError(\"Cannot differentiate a function that returns None; \"\n",
      "\u001b[0;32m<ipython-input-28-8b7ae87d3699>\u001b[0m in \u001b[0;36mloss_fn\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimplicit_value_and_gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/tensorflow/python/layers/base.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             raise ValueError('A layer\\'s `call` method should return a Tensor '\n",
      "\u001b[0;32m/mnt/nfs-east1-d/work/tk/vendor/tensor2tensor/tensor2tensor/utils/t2t_model.pyc\u001b[0m in \u001b[0;36mcall\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    157\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fill_problem_hparams_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m       \u001b[0msharded_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shard_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m       \u001b[0msharded_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_fn_sharded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msharded_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msharded_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mconcat_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/nfs-east1-d/work/tk/vendor/tensor2tensor/tensor2tensor/utils/t2t_model.pyc\u001b[0m in \u001b[0;36mmodel_fn_sharded\u001b[0;34m(self, sharded_features)\u001b[0m\n\u001b[1;32m    212\u001b[0m           \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loss_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m       \u001b[0msharded_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharded_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatashard_to_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msharded_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mtemp_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msharded_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/nfs-east1-d/work/tk/vendor/tensor2tensor/tensor2tensor/utils/expert_utils.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_devices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mDEFAULT_DEV_STRING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_devices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m               \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmy_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmy_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmy_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmy_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/nfs-east1-d/work/tk/vendor/tensor2tensor/tensor2tensor/utils/t2t_model.pyc\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    246\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"body\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mlog_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Building model body\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mbody_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m       \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalize_body_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-b3847348ec43>\u001b[0m in \u001b[0;36mbody\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mcfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_from_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mcfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcode_from_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/nfs-east1-d/work/tk/vendor/tensor2tensor/tensor2tensor/utils/t2t_model.pyc\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, logits, features)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \"since problem_hparams.target_modality is a dict.\")\n\u001b[1;32m    416\u001b[0m         \u001b[0mtarget_modality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_modality\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"targets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_modality\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"targets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_async_replicas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/nfs-east1-d/work/tk/vendor/tensor2tensor/tensor2tensor/utils/t2t_model.pyc\u001b[0m in \u001b[0;36m_loss_single\u001b[0;34m(self, logits, target_modality, feature)\u001b[0m\n\u001b[1;32m    387\u001b[0m               tf.constant(1., dtype=tf.float32))\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0mloss_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_den\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_modality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m     \u001b[0mloss_num\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_problem_hparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_den\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/nfs-east1-d/work/tk/vendor/tensor2tensor/tensor2tensor/utils/modality.pyc\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, top_out, targets)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_hparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         weights_fn=self.targets_weights_fn)\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mloss_sharded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharded_top_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharded_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_parallelism\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/nfs-east1-d/work/tk/vendor/tensor2tensor/tensor2tensor/layers/common_layers.pyc\u001b[0m in \u001b[0;36mpadded_cross_entropy\u001b[0;34m(logits, labels, label_smoothing, weights_fn, reduce_sum, cutoff, gaussian)\u001b[0m\n\u001b[1;32m   1849\u001b[0m       \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_with_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m     logits = tf.reshape(logits, shape_list(labels) + [vocab_size],\n\u001b[0;32m-> 1851\u001b[0;31m                         name=\"padded_cross_entropy_size_check\")\n\u001b[0m\u001b[1;32m   1852\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m     xent = smoothing_cross_entropy(logits, labels, vocab_size, confidence,\n",
      "\u001b[0;32m/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.pyc\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   3911\u001b[0m     \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tshape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_Tshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3912\u001b[0m     _result = _execute.execute(b\"Reshape\", 1, inputs=_inputs_flat,\n\u001b[0;32m-> 3913\u001b[0;31m                                attrs=_attrs, ctx=_ctx, name=name)\n\u001b[0m\u001b[1;32m   3914\u001b[0m   _execute.record_gradient(\n\u001b[1;32m   3915\u001b[0m       \"Reshape\", _inputs_flat, _attrs, _result, name)\n",
      "\u001b[0;32m/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/tensorflow/python/eager/execute.pyc\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jovyan/.conda/envs/py2/lib/python2.7/site-packages/six.pyc\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 271722 values, but the requested shape has 34780416 [Op:Reshape] name: padded_cross_entropy_size_check"
     ]
    }
   ],
   "source": [
    "  \n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "@tfe.implicit_value_and_gradients\n",
    "def loss_fn(features):\n",
    "    _, losses = model(features)\n",
    "    return losses[\"training\"]\n",
    "\n",
    "NUM_STEPS = 10\n",
    "\n",
    "for count, example in enumerate(iterator):\n",
    "    loss, gv = loss_fn(example)\n",
    "    optimizer.apply_gradients(gv)\n",
    "\n",
    "    if count % 1 == 0:\n",
    "        print(\"Step: %d, Loss: %.3f\" % (count, loss.numpy()))\n",
    "    if count >= NUM_STEPS:\n",
    "       break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
